{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module and Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T15:42:13.710654Z",
     "start_time": "2021-02-26T15:42:13.704671Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import calendar\n",
    "\n",
    "import sklearn\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "\n",
    "from sklearn import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "\n",
    "# from utils_model import * # expand later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T15:42:15.796178Z",
     "start_time": "2021-02-26T15:42:15.783539Z"
    }
   },
   "outputs": [],
   "source": [
    "def fast_build_model_FE(X,y,cv,Feature_Engineering,parameters, model_base=LogisticRegression(class_weight='balanced'),random_state=0,shuffle=False):\n",
    "\n",
    "    num_transformer = Pipeline(steps=[\n",
    "                                    ('imputer', SimpleImputer(strategy = 'median')),\n",
    "                                    ('scaler', RobustScaler())\n",
    "                                    ])\n",
    "\n",
    "    cat_transformer = Pipeline(steps=[\n",
    "                                    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                                    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "                                    ])\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=cv,random_state = random_state,shuffle = shuffle)\n",
    "\n",
    "\n",
    "    # oof validation\n",
    "    oof_y_valid = []\n",
    "    oof_y_valid_pred = []\n",
    "    oof_y_valid_pred_proba = []\n",
    "    pipelines = []\n",
    "    add_fes = []\n",
    "    data = pd.concat([X,y],axis=1)\n",
    "    aucs=[]\n",
    "    \n",
    "    for cv,(train_index, val_index) in enumerate(skf.split(X,y)):\n",
    "        start_fit = time.time()\n",
    "        data_train = data.iloc[train_index,:].copy()\n",
    "        \n",
    "        add_fe = Feature_Engineering(parameters)\n",
    "        add_fe.fit(data_train)\n",
    "        \n",
    "        X_train = add_fe.transform(data_train).drop(columns=[parameters['target']])\n",
    "        y_train = y.iloc[train_index]\n",
    "        X_val = add_fe.transform(X.iloc[val_index,:],mode='val')\n",
    "        y_val = y.iloc[val_index]\n",
    "        \n",
    "        num_cols_fe = list(X_train.select_dtypes(exclude='object').columns)\n",
    "        cat_cols_fe = list(X_train.select_dtypes(include='object').columns)        \n",
    "        \n",
    "#         print(X_train.shape,X_val.shape,data.shape)\n",
    "        \n",
    "        transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', num_transformer, num_cols_fe),\n",
    "            ('cat', cat_transformer, cat_cols_fe)\n",
    "        ])\n",
    "        \n",
    "        main_pipeline = Pipeline(steps=[('transformer', transformer),\n",
    "                          ('classifier', model_base)])\n",
    "        \n",
    "\n",
    "        \n",
    "        add_fes.append(add_fe)\n",
    "        model = clone(main_pipeline)\n",
    "        model.fit(X_train,y_train.values.ravel())\n",
    "        pred = model.predict(X_val)\n",
    "        pred_proba = model.predict_proba(X_val)[:,1]\n",
    "        oof_y_valid_pred.extend(pred)\n",
    "        oof_y_valid_pred_proba.extend(pred_proba)\n",
    "        oof_y_valid.extend(y_val.values)\n",
    "        aucs.append(roc_auc_score(y_val.values, pred_proba,average='weighted'))\n",
    "        pipelines.append(model)\n",
    "        print(f'Fit iteration {cv} done in : {str(time.time()-start_fit)}')\n",
    "\n",
    "    prec,rec,f1, _ = precision_recall_fscore_support(oof_y_valid,oof_y_valid_pred)\n",
    "    auc = roc_auc_score(oof_y_valid, oof_y_valid_pred_proba,average='weighted')\n",
    "    print(f'PRec Rec AUC average : {prec} {rec} <==> {auc}')\n",
    "    print(aucs)\n",
    "    print(np.mean(aucs[:2]))\n",
    "    return add_fes,pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T15:42:16.458823Z",
     "start_time": "2021-02-26T15:42:16.453825Z"
    }
   },
   "outputs": [],
   "source": [
    "def fast_predict_FE(data,add_fes,pipelines):\n",
    "    X = data.copy()\n",
    "#     pred = np.zeros(1,len(X))\n",
    "    pred_proba = np.zeros((len(X)))\n",
    "    dude={}\n",
    "    for i in range(len(pipelines)):\n",
    "        \n",
    "        pred_proba += pipelines[i].predict_proba(add_fes[i].transform(X,mode='test'))[:,1] / len(pipelines)\n",
    "        dude[i] = pipelines[i].predict_proba(add_fes[i].transform(X,mode='test'))[:,1]\n",
    "    \n",
    "    return pred_proba,dude\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T15:42:16.821870Z",
     "start_time": "2021-02-26T15:42:16.783969Z"
    }
   },
   "outputs": [],
   "source": [
    "class Feature_Engineering:\n",
    "    def __init__(self,parameters):\n",
    "        self.parameters = parameters\n",
    "        self.target = parameters['target']\n",
    "    \n",
    "    @staticmethod  \n",
    "    def check_col(col):\n",
    "        if len(col.split(' '))>1:\n",
    "            col2 = '_'.join(col.split(' '))\n",
    "        else:\n",
    "            col2 = col\n",
    "        return col2\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_bin(data,col,n_bin,mode='cut'):\n",
    "        while True:\n",
    "            try:\n",
    "                if mode=='cut':\n",
    "                    _,bin_dummy = pd.cut(data[col],n_bin,retbins=True)\n",
    "                else:\n",
    "                    _,bin_dummy = pd.qcut(data[col],n_bin,retbins=True)\n",
    "            except:\n",
    "                n_bin -= 1\n",
    "                continue\n",
    "            break\n",
    "        return bin_dummy\n",
    "        \n",
    "    def fit(self,data_ori):\n",
    "        target = self.target\n",
    "        data = data_ori.copy()\n",
    "        for param in self.parameters['bin_numer_qcut']:\n",
    "            col = param[0]\n",
    "            n_bin = param[1]\n",
    "            bin_dummy = self.get_bin(data,col,n_bin,mode='qcut')\n",
    "            bin_dummy[0] = bin_dummy[0]-0.001\n",
    "            bin_dummy[-1] = np.inf\n",
    "            setattr(self,f'{col}_bin_numer_qcut',bin_dummy)\n",
    "        for param in self.parameters['bin_numer_cut']:\n",
    "            col = param[0]\n",
    "            n_bin = param[1]\n",
    "            bin_dummy = self.get_bin(data,col,n_bin,mode='cut')\n",
    "            bin_dummy[0] = bin_dummy[0]-0.001\n",
    "            bin_dummy[-1] = np.inf\n",
    "            setattr(self,f'{col}_bin_numer_cut',bin_dummy)\n",
    "            \n",
    "            \n",
    "        for param in self.parameters['bin_add_categ_numer_bin_cut']:\n",
    "            col = param[1]\n",
    "            n_bin = param[2]\n",
    "            bin_dummy = self.get_bin(data,col,n_bin,mode='cut')\n",
    "            bin_dummy[0] = bin_dummy[0]-0.001\n",
    "            bin_dummy[-1] = np.inf\n",
    "            setattr(self,f'{col}_bin_cut_add_categ',bin_dummy)\n",
    "            \n",
    "        for param in self.parameters['bin_add_categ_numer_bin_qcut']:\n",
    "            col = param[1]\n",
    "            n_bin = param[2]\n",
    "            bin_dummy = self.get_bin(data,col,n_bin,mode='qcut')\n",
    "            bin_dummy[0] = bin_dummy[0]-0.001\n",
    "            bin_dummy[-1] = np.inf\n",
    "            setattr(self,f'{col}_bin_qcut_add_categ',bin_dummy)\n",
    "        \n",
    "        for param in self.parameters['bin_target_encoding_cut']:\n",
    "            col = param[0]\n",
    "            n_bin = param[1]\n",
    "            bin_dummy = self.get_bin(data,col,n_bin,mode='cut')\n",
    "            bin_dummy[0] = bin_dummy[0]-0.001\n",
    "            bin_dummy[-1] = np.inf\n",
    "            setattr(self,f'{col}_bin_cut',bin_dummy)\n",
    "            \n",
    "            data[f'{col}_bin_target_encoding_cut'] = pd.cut(data[col],bins=bin_dummy)\n",
    "            data_dummy = data.groupby([f'{col}_bin_target_encoding_cut'])[target].mean().reset_index(drop=False)\n",
    "            setattr(self,f'{col}_bin_target_encoding_cut',data_dummy)\n",
    "            \n",
    "        for param in self.parameters['bin_target_encoding_qcut']:\n",
    "            col = param[0]\n",
    "            n_bin = param[1]\n",
    "            bin_dummy = self.get_bin(data,col,n_bin,mode='qcut')\n",
    "            bin_dummy[0] = bin_dummy[0]-0.001\n",
    "            bin_dummy[-1] = np.inf\n",
    "            setattr(self,f'{col}_bin_qcut',bin_dummy)\n",
    "            \n",
    "            data[f'{col}_bin_target_encoding_qcut'] = pd.cut(data[col],bins=bin_dummy)\n",
    "            data_dummy = data.groupby([f'{col}_bin_target_encoding_qcut'])[target].mean().reset_index(drop=False)\n",
    "            setattr(self,f'{col}_bin_target_encoding_qcut',data_dummy)\n",
    "           \n",
    "        for param in self.parameters['bin_target_encoding_custom_bin']:\n",
    "            col = param[0]\n",
    "            bins = param[1]\n",
    "            setattr(self,f'{col}_bin_custom_bin',bins)\n",
    "            \n",
    "            data[f'{col}_bin_target_encoding_custom_bin'] = pd.cut(data[col],bins=bins)\n",
    "            data_dummy = data.groupby([f'{col}_bin_target_encoding_custom_bin'])[target].mean().reset_index(drop=False)\n",
    "            setattr(self,f'{col}_bin_target_encoding_custom_bin',data_dummy)\n",
    "        \n",
    "        for param in self.parameters['categorical_mean_encoding']:\n",
    "            col = param\n",
    "            data[f'{col}_categorical_mean_encoding'] = data[col].copy().values\n",
    "            data_dummy = data.groupby([f'{col}_categorical_mean_encoding'])[target].mean().reset_index(drop=False)\n",
    "            setattr(self,f'{col}_categorical_mean_encoding',data_dummy)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fit = True\n",
    "        return data\n",
    "        \n",
    "    \n",
    "    def transform(self,X,mode='train'):\n",
    "        data = X.copy()\n",
    "        target = self.target\n",
    "        if mode!='train':\n",
    "            target_encode = self.target\n",
    "        else:\n",
    "            target_encode = self.target +\"_y\"\n",
    "            \n",
    "        if self.fit==False:\n",
    "            raise Exception(\"Fit to train data first\")\n",
    "        \n",
    "        for param in self.parameters['bin_numer_qcut']:\n",
    "            col = param[0]\n",
    "            bin_dummy = eval(f'self.{col}_bin_numer_qcut')\n",
    "            data[f'{col}_bin_numer_qcut'] = pd.cut(data[col],bins=bin_dummy).astype(str).values\n",
    "        for param in self.parameters['bin_numer_cut']:\n",
    "            col = param[0]\n",
    "            bin_dummy = eval(f'self.{col}_bin_numer_cut')\n",
    "            data[f'{col}_bin_numer_cut'] = pd.cut(data[col],bins=bin_dummy).astype(str).values\n",
    "            \n",
    "        for cols in self.parameters['bin_add_categ_numer_bin_cut']:\n",
    "            col_add = cols[0] + '_' + cols[1]\n",
    "            bin_dummy = eval(f'self.{cols[1]}_bin_cut_add_categ')\n",
    "            data[f'{col_add}_bin_add_categ_numer_bin_cut'] = pd.cut(data[cols[1]],bins=bin_dummy).values\n",
    "            data[f'{col_add}_bin_add_categ_numer_bin_cut'] = (data[cols[0]].astype(str)+'_' + data[f'{col_add}_bin_add_categ_numer_bin_cut'].astype(str)).values\n",
    "            \n",
    "        for cols in self.parameters['bin_add_categ_numer_bin_qcut']:\n",
    "            col_add = cols[0] + '_' + cols[1]\n",
    "            bin_dummy = eval(f'self.{cols[1]}_bin_qcut_add_categ')\n",
    "            data[f'{col_add}_bin_add_categ_numer_bin_qcut'] = pd.cut(data[cols[1]],bins=bin_dummy).values\n",
    "            data[f'{col_add}_bin_add_categ_numer_bin_qcut'] = (data[cols[0]].astype(str)+'_' + data[f'{col_add}_bin_add_categ_numer_bin_qcut'].astype(str)).values\n",
    "        \n",
    "        for param in self.parameters['bin_target_encoding_cut']:\n",
    "            col = param[0]\n",
    "            bin_dummy = eval(f'self.{col}_bin_cut')\n",
    "            data_dummy = eval(f'self.{col}_bin_target_encoding_cut')\n",
    "            data[f'{col}_bin_target_encoding_cut'] = pd.cut(data[col],bins=bin_dummy).values\n",
    "            data[f'{col}_bin_target_encoding_cut'] = pd.merge(data,data_dummy,how='left',on=[f'{col}_bin_target_encoding_cut'])[f'{target_encode}'].values\n",
    "        \n",
    "        for param in self.parameters['bin_target_encoding_qcut']:\n",
    "            col = param[0]\n",
    "            bin_dummy = eval(f'self.{col}_bin_qcut')\n",
    "            data_dummy = eval(f'self.{col}_bin_target_encoding_qcut')\n",
    "            data[f'{col}_bin_target_encoding_qcut'] = pd.cut(data[col],bins=bin_dummy).values\n",
    "            data[f'{col}_bin_target_encoding_qcut'] = pd.merge(data,data_dummy,how='left',on=[f'{col}_bin_target_encoding_qcut'])[f'{target_encode}'].values\n",
    "        \n",
    "        for param in self.parameters['bin_target_encoding_custom_bin']:\n",
    "            col = param[0]\n",
    "            bin_dummy = eval(f'self.{col}_bin_custom_bin')\n",
    "            data_dummy = eval(f'self.{col}_bin_target_encoding_custom_bin')\n",
    "            data[f'{col}_bin_target_encoding_custom_bin'] = pd.cut(data[col],bins=bin_dummy).values\n",
    "            data[f'{col}_bin_target_encoding_custom_bin'] = pd.merge(data,data_dummy,how='left',on=[f'{col}_bin_target_encoding_custom_bin'])[f'{target_encode}'].values\n",
    "        \n",
    "        for param in self.parameters['categorical_mean_encoding']:\n",
    "            col = param\n",
    "            data_dummy = eval(f'self.{col}_categorical_mean_encoding')\n",
    "            data[f'{col}_categorical_mean_encoding'] = data[col].copy().values\n",
    "            data[f'{col}_categorical_mean_encoding'] = pd.merge(data,data_dummy,how='left',on=[f'{col}_categorical_mean_encoding'])[f'{target_encode}'].values\n",
    "        \n",
    "        \n",
    "        for cols in self.parameters['multiply']:\n",
    "            data[cols[0] + '_times_' +cols[1]] = (data[cols[0]] * data[cols[1]]).values\n",
    "        for cols in self.parameters['add']:\n",
    "            data[cols[0] + '_plus_' +cols[1]] = (data[cols[0]] + data[cols[1]]).values\n",
    "        for cols in self.parameters['add_str']:\n",
    "            data[cols[0] + '_plus_' +cols[1]] = (data[cols[0]].astype(str)+'_' + data[cols[1]].astype(str)).values\n",
    "            \n",
    "        for cols in self.parameters['substract']:\n",
    "            data[cols[0] + '_minus_' +cols[1]] = (data[cols[0]] - data[cols[1]]).values\n",
    "        for cols in self.parameters['divide']:\n",
    "            data[cols[0] + '_divide_' +cols[1]] = (data[cols[0]] / np.where(data[cols[1]]==0,0.0001,data[cols[1]])).values\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T15:58:00.804493Z",
     "start_time": "2021-02-26T15:58:00.702734Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "LE = LabelEncoder()\n",
    "\n",
    "data = pd.read_csv('train.csv')\n",
    "data_test = pd.read_csv('test.csv')\n",
    "\n",
    "data = data.rename(columns={'Last_achievement_%':'Last_achievement','marital_status_maried(Y/N)':'marital_status_maried',\n",
    "                           'Achievement_above_100%_during3quartal':'Achievement_above_100_during3quartal'})\n",
    "data_test = data_test.rename(columns={'Last_achievement_%':'Last_achievement','marital_status_maried(Y/N)':'marital_status_maried',\n",
    "                           'Achievement_above_100%_during3quartal':'Achievement_above_100_during3quartal'})\n",
    "\n",
    "data['gender_str'] = data['gender'].astype('str')\n",
    "data_test['gender_str'] = data_test['gender'].astype('str')\n",
    "\n",
    "\n",
    "data['Achievement_above_100_during3quartal_str'] = data['Achievement_above_100_during3quartal'].astype(str)\n",
    "data_test['Achievement_above_100_during3quartal_str'] = data_test['Achievement_above_100_during3quartal'].astype(str)\n",
    "\n",
    "data = data.rename(columns={'annual leave':'annual_leave'})\n",
    "data_test = data_test.rename(columns={'annual leave':'annual_leave'})\n",
    "\n",
    "data['person_level_ordinary'] = LE.fit_transform(data['person_level'])\n",
    "data.drop\n",
    "data['job_level_ordinary'] = LE.fit_transform(data['job_level'])\n",
    "data['Education_level_ordinary'] = LE.fit_transform(data['Education_level'])\n",
    "\n",
    "data_test['person_level_ordinary'] = LE.fit_transform(data_test['person_level'])\n",
    "data_test['job_level_ordinary'] = LE.fit_transform(data_test['job_level'])\n",
    "data_test['Education_level_ordinary'] = LE.fit_transform(data_test['Education_level'])\n",
    "\n",
    "\n",
    "\n",
    "drop = ['age', 'job_duration_from_training', 'person_level_ordinary', 'job_level']\n",
    "\n",
    "\n",
    "data = data.drop(columns=drop)\n",
    "data_test = data_test.drop(columns=drop)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T15:58:02.299233Z",
     "start_time": "2021-02-26T15:58:02.291247Z"
    }
   },
   "outputs": [],
   "source": [
    "# get test data (for final evaluation)\n",
    "X_train = data.drop(columns=['Best Performance'])\n",
    "y_train = data['Best Performance']\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T15:58:02.743088Z",
     "start_time": "2021-02-26T15:58:02.739098Z"
    }
   },
   "outputs": [],
   "source": [
    "# len(X_train),len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T15:58:03.102160Z",
     "start_time": "2021-02-26T15:58:03.091189Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 6, 22)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cols = list(X_train.select_dtypes(exclude=['object']))\n",
    "cat_cols = list(X_train.select_dtypes(include=['object']))\n",
    "features = list(X_train.columns)\n",
    "len(num_cols),len(cat_cols),len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T15:58:04.360848Z",
     "start_time": "2021-02-26T15:58:04.353862Z"
    }
   },
   "outputs": [],
   "source": [
    "parameters = {'multiply':[['job_duration_in_current_job_level','number_of_dependences'],\n",
    "                         ['job_duration_in_current_job_level','job_rotation'],\n",
    "                         ['job_duration_in_current_job_level','job_level_ordinary'],\n",
    "                         ['gender','GPA'],['gender','year_graduated'],['gender','sick_leaves'],['gender','job_level_ordinary'],\n",
    "                         ['number_of_dependences','year_graduated'],['number_of_dependences','annual_leave'],['number_of_dependences','job_level_ordinary'],\n",
    "                         ['GPA','branch_rotation'],['year_graduated','job_level_ordinary']],\n",
    "              'add':[],\n",
    "              'add_str':[['person_level','Education_level'],['Employee_type','Education_level']],\n",
    "              'substract':[],\n",
    "              'divide':[],\n",
    "              'bin_numer_qcut':[['assign_of_otherposition',10],['Last_achievement',10]],\n",
    "              'bin_numer_cut':[['Last_achievement',20]],\n",
    "              'bin_add_categ_numer_bin_qcut':[],\n",
    "              'bin_add_categ_numer_bin_cut':[],\n",
    "            'bin_target_encoding_cut':[['job_duration_in_current_job_level',10],['assign_of_otherposition',20]],\n",
    "             'bin_target_encoding_qcut':[['job_duration_in_current_branch',10]\n",
    "                                        ],\n",
    "             'bin_target_encoding_custom_bin':[],\n",
    "              'categorical_mean_encoding':[],\n",
    "             'target':'Best Performance'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T16:02:11.362269Z",
     "start_time": "2021-02-26T16:01:41.789380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit iteration 0 done in : 5.138310432434082\n",
      "Fit iteration 1 done in : 7.602585792541504\n",
      "Fit iteration 2 done in : 4.447324752807617\n",
      "Fit iteration 3 done in : 6.385558843612671\n",
      "Fit iteration 4 done in : 5.9272685050964355\n",
      "PRec Rec AUC average : [0.87854624 0.1824266 ] [0.60203867 0.51618815] <==> 0.5798468733365636\n",
      "[0.6027211343252896, 0.58635594632352, 0.5845904412801354, 0.575187093933448, 0.5546047525153428]\n",
      "0.5945385403244048\n"
     ]
    }
   ],
   "source": [
    "cv=5\n",
    "add_fes,pipelines = fast_build_model_FE(X_train,y_train,cv,\n",
    "                    Feature_Engineering,parameters,model_base=LogisticRegression(class_weight='balanced',random_state=0,max_iter=3000,C=1),shuffle=True,random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T15:56:47.247508Z",
     "start_time": "2021-02-26T15:56:44.475968Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Best Performance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.537903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.666084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.770454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.555947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.702825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>5995</td>\n",
       "      <td>0.587593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>5996</td>\n",
       "      <td>0.572165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>5997</td>\n",
       "      <td>0.537532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>5998</td>\n",
       "      <td>0.537482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>5999</td>\n",
       "      <td>0.645849</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index  Best Performance\n",
       "0         0          0.537903\n",
       "1         1          0.666084\n",
       "2         2          0.770454\n",
       "3         3          0.555947\n",
       "4         4          0.702825\n",
       "...     ...               ...\n",
       "5995   5995          0.587593\n",
       "5996   5996          0.572165\n",
       "5997   5997          0.537532\n",
       "5998   5998          0.537482\n",
       "5999   5999          0.645849\n",
       "\n",
       "[6000 rows x 2 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_proba,_ = fast_predict_FE(data_test.copy(),add_fes,pipelines)\n",
    "\n",
    "df_submission = pd.DataFrame({'index':data_test.index,'Best Performance':pred_proba})\n",
    "df_submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-26T16:03:21.236368Z",
     "start_time": "2021-02-26T16:03:21.211398Z"
    }
   },
   "outputs": [],
   "source": [
    "df_submission.to_csv('answer_submission_1.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
