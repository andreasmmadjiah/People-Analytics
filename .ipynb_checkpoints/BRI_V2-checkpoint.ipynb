{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T14:41:07.158517Z",
     "start_time": "2021-02-15T14:41:07.149547Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import calendar\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, RobustScaler\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "\n",
    "\n",
    "from sklearn import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# from utils_model import * # expand later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T14:56:30.421044Z",
     "start_time": "2021-02-15T14:56:30.338015Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')\n",
    "data_test = pd.read_csv('test.csv')\n",
    "\n",
    "data['gender'] = data['gender'].astype('str')\n",
    "data_test['gender'] = data_test['gender'].astype('str')\n",
    "\n",
    "data['Achievement_above_100%_during3quartal'] = data['Achievement_above_100%_during3quartal'].astype(str)\n",
    "data_test['Achievement_above_100%_during3quartal'] = data_test['Achievement_above_100%_during3quartal'].astype(str)\n",
    "\n",
    "\n",
    "data = data.rename(columns={'annual leave':'annual_leave'})\n",
    "data_test = data_test.rename(columns={'annual leave':'annual_leave'})\n",
    "\n",
    "data = data.rename(columns={'Last_achievement_%':'Last_achievement'})\n",
    "data_test = data_test.rename(columns={'Last_achievement_%':'Last_achievement'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-14T06:51:32.232641Z",
     "start_time": "2021-02-14T06:51:32.222612Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T14:56:30.715592Z",
     "start_time": "2021-02-15T14:56:30.706625Z"
    }
   },
   "outputs": [],
   "source": [
    "# get test data (for final evaluation)\n",
    "X = data.drop(columns=['Best Performance'])\n",
    "y = data['Best Performance']\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T14:56:30.887094Z",
     "start_time": "2021-02-15T14:56:30.873066Z"
    }
   },
   "outputs": [],
   "source": [
    "# len(X_train),len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T14:56:31.043469Z",
     "start_time": "2021-02-15T14:56:31.025472Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 7, 21)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cols = list(X_train.select_dtypes(exclude=['object']))\n",
    "cat_cols = list(X_train.select_dtypes(include=['object']))\n",
    "features = list(X_train.columns)\n",
    "len(num_cols),len(cat_cols),len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T14:56:31.184128Z",
     "start_time": "2021-02-15T14:56:31.178129Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['boosting_type',\n",
       " 'num_leaves',\n",
       " 'max_depth',\n",
       " 'learning_rate',\n",
       " 'n_estimators',\n",
       " 'subsample_for_bin',\n",
       " 'objective',\n",
       " 'class_weight',\n",
       " 'min_split_gain',\n",
       " 'min_child_weight',\n",
       " 'min_child_samples',\n",
       " 'subsample',\n",
       " 'subsample_freq',\n",
       " 'colsample_bytree',\n",
       " 'reg_alpha',\n",
       " 'reg_lambda',\n",
       " 'random_state',\n",
       " '']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "a = '''boosting_type='gbdt',\n",
    "    num_leaves=31,\n",
    "    max_depth=-1,\n",
    "    learning_rate=0.1,\n",
    "    n_estimators=100,\n",
    "    subsample_for_bin=200000,\n",
    "    objective=None,\n",
    "    class_weight=None,\n",
    "    min_split_gain=0.0,\n",
    "    min_child_weight=0.001,\n",
    "    min_child_samples=20,\n",
    "    subsample=1.0,\n",
    "    subsample_freq=0,\n",
    "    colsample_bytree=1.0,\n",
    "    reg_alpha=0.0,\n",
    "    reg_lambda=0.0,\n",
    "    random_state=None\n",
    "    '''\n",
    "# [x.strip() for x in a.split(',\\n')]\n",
    "[x.split('=')[0].strip() for x in a.split('\\n')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T14:57:36.345803Z",
     "start_time": "2021-02-15T14:57:36.258807Z"
    }
   },
   "outputs": [],
   "source": [
    "class Feature_Engineering:\n",
    "    def __init__(self,parameters):\n",
    "        self.parameters = parameters\n",
    "        self.target = parameters['target']\n",
    "    \n",
    "    @staticmethod  \n",
    "    def check_col(col):\n",
    "        if len(col.split(' '))>1:\n",
    "            col2 = '_'.join(col.split(' '))\n",
    "        else:\n",
    "            col2 = col\n",
    "        return col2\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_bin(data,col,n_bin,mode='cut'):\n",
    "        while True:\n",
    "            try:\n",
    "                if mode=='cut':\n",
    "                    _,bin_dummy = pd.cut(data[col],n_bin,retbins=True)\n",
    "                else:\n",
    "                    _,bin_dummy = pd.qcut(data[col],n_bin,retbins=True)\n",
    "            except:\n",
    "                n_bin -= 1\n",
    "                continue\n",
    "            break\n",
    "        return bin_dummy\n",
    "        \n",
    "    def fit(self,data_ori):\n",
    "        target = self.target\n",
    "        data = data_ori.copy()\n",
    "        for param in self.parameters['bin_numer_qcut']:\n",
    "            col = param[0]\n",
    "            n_bin = param[1]\n",
    "            bin_dummy = self.get_bin(data,col,n_bin,mode='qcut')\n",
    "            bin_dummy[0] = bin_dummy[0]-0.001\n",
    "            bin_dummy[-1] = np.inf\n",
    "            setattr(self,f'{col}_bin_numer_qcut',bin_dummy)\n",
    "        for param in self.parameters['bin_numer_cut']:\n",
    "            col = param[0]\n",
    "            n_bin = param[1]\n",
    "            bin_dummy = self.get_bin(data,col,n_bin,mode='cut')\n",
    "            bin_dummy[0] = bin_dummy[0]-0.001\n",
    "            bin_dummy[-1] = np.inf\n",
    "            setattr(self,f'{col}_bin_numer_cut',bin_dummy)\n",
    "            \n",
    "            \n",
    "        for param in self.parameters['bin_add_categ_numer_bin_qcut']:\n",
    "            col = param[1]\n",
    "            n_bin = param[2]\n",
    "            bin_dummy = self.get_bin(data,col,n_bin,mode='qcut')\n",
    "            bin_dummy[0] = bin_dummy[0]-0.001\n",
    "            bin_dummy[-1] = np.inf\n",
    "            setattr(self,f'{col}_bin_qcut_add_categ',bin_dummy)\n",
    "        \n",
    "        for param in self.parameters['bin_target_encoding_cut']:\n",
    "            col = param[0]\n",
    "            n_bin = param[1]\n",
    "            bin_dummy = self.get_bin(data,col,n_bin,mode='cut')\n",
    "            bin_dummy[0] = bin_dummy[0]-0.001\n",
    "            bin_dummy[-1] = np.inf\n",
    "            setattr(self,f'{col}_bin_cut',bin_dummy)\n",
    "            \n",
    "            data[f'{col}_bin_target_encoding_cut'] = pd.cut(data[col],bins=bin_dummy)\n",
    "            data_dummy = data.groupby([f'{col}_bin_target_encoding_cut'])[target].mean().reset_index(drop=False)\n",
    "            setattr(self,f'{col}_bin_target_encoding_cut',data_dummy)\n",
    "            \n",
    "        for param in self.parameters['bin_target_encoding_qcut']:\n",
    "            col = param[0]\n",
    "            n_bin = param[1]\n",
    "            bin_dummy = self.get_bin(data,col,n_bin,mode='qcut')\n",
    "            bin_dummy[0] = bin_dummy[0]-0.001\n",
    "            bin_dummy[-1] = np.inf\n",
    "            setattr(self,f'{col}_bin_qcut',bin_dummy)\n",
    "            \n",
    "            data[f'{col}_bin_target_encoding_qcut'] = pd.cut(data[col],bins=bin_dummy)\n",
    "            data_dummy = data.groupby([f'{col}_bin_target_encoding_qcut'])[target].mean().reset_index(drop=False)\n",
    "            setattr(self,f'{col}_bin_target_encoding_qcut',data_dummy)\n",
    "           \n",
    "        for param in self.parameters['bin_target_encoding_custom_bin']:\n",
    "            col = param[0]\n",
    "            bins = param[1]\n",
    "            setattr(self,f'{col}_bin_custom_bin',bins)\n",
    "            \n",
    "            data[f'{col}_bin_target_encoding_custom_bin'] = pd.cut(data[col],bins=bins)\n",
    "            data_dummy = data.groupby([f'{col}_bin_target_encoding_custom_bin'])[target].mean().reset_index(drop=False)\n",
    "            setattr(self,f'{col}_bin_target_encoding_custom_bin',data_dummy)\n",
    "        \n",
    "        for param in self.parameters['categorical_mean_encoding']:\n",
    "            col = param\n",
    "            data[f'{col}_categorical_mean_encoding'] = data[col].copy().values\n",
    "            data_dummy = data.groupby([f'{col}_categorical_mean_encoding'])[target].mean().reset_index(drop=False)\n",
    "            setattr(self,f'{col}_categorical_mean_encoding',data_dummy)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fit = True\n",
    "        return data\n",
    "        \n",
    "    \n",
    "    def transform(self,X,mode='train'):\n",
    "        data = X.copy()\n",
    "        target = self.target\n",
    "        if mode!='train':\n",
    "            target_encode = self.target\n",
    "        else:\n",
    "            target_encode = self.target +\"_y\"\n",
    "            \n",
    "        if self.fit==False:\n",
    "            raise Exception(\"Fit to train data first\")\n",
    "        \n",
    "        for param in self.parameters['bin_numer_qcut']:\n",
    "            col = param[0]\n",
    "            bin_dummy = eval(f'self.{col}_bin_numer_qcut')\n",
    "            data[f'{col}_bin_numer_qcut'] = pd.cut(data[col],bins=bin_dummy).astype(str).values\n",
    "        for param in self.parameters['bin_numer_cut']:\n",
    "            col = param[0]\n",
    "            bin_dummy = eval(f'self.{col}_bin_numer_cut')\n",
    "            data[f'{col}_bin_numer_cut'] = pd.cut(data[col],bins=bin_dummy).astype(str).values\n",
    "            \n",
    "        for cols in self.parameters['bin_add_categ_numer_bin_qcut']:\n",
    "            col_add = cols[0] + '_' + cols[1]\n",
    "            bin_dummy = eval(f'self.{cols[1]}_bin_qcut_add_categ')\n",
    "            data[f'{col_add}_bin_add_categ_numer_bin_qcut'] = pd.cut(data[cols[1]],bins=bin_dummy).values\n",
    "            data[f'{col_add}_bin_add_categ_numer_bin_qcut'] = (data[cols[0]].astype(str)+'_' + data[f'{col_add}_bin_add_categ_numer_bin_qcut'].astype(str)).values\n",
    "        \n",
    "        for param in self.parameters['bin_target_encoding_cut']:\n",
    "            col = param[0]\n",
    "            bin_dummy = eval(f'self.{col}_bin_cut')\n",
    "            data_dummy = eval(f'self.{col}_bin_target_encoding_cut')\n",
    "            data[f'{col}_bin_target_encoding_cut'] = pd.cut(data[col],bins=bin_dummy).values\n",
    "            data[f'{col}_bin_target_encoding_cut'] = pd.merge(data,data_dummy,how='left',on=[f'{col}_bin_target_encoding_cut'])[f'{target_encode}'].values\n",
    "        \n",
    "        for param in self.parameters['bin_target_encoding_qcut']:\n",
    "            col = param[0]\n",
    "            bin_dummy = eval(f'self.{col}_bin_qcut')\n",
    "            data_dummy = eval(f'self.{col}_bin_target_encoding_qcut')\n",
    "            data[f'{col}_bin_target_encoding_qcut'] = pd.cut(data[col],bins=bin_dummy).values\n",
    "            data[f'{col}_bin_target_encoding_qcut'] = pd.merge(data,data_dummy,how='left',on=[f'{col}_bin_target_encoding_qcut'])[f'{target_encode}'].values\n",
    "        \n",
    "        for param in self.parameters['bin_target_encoding_custom_bin']:\n",
    "            col = param[0]\n",
    "            bin_dummy = eval(f'self.{col}_bin_custom_bin')\n",
    "            data_dummy = eval(f'self.{col}_bin_target_encoding_custom_bin')\n",
    "            data[f'{col}_bin_target_encoding_custom_bin'] = pd.cut(data[col],bins=bin_dummy).values\n",
    "            data[f'{col}_bin_target_encoding_custom_bin'] = pd.merge(data,data_dummy,how='left',on=[f'{col}_bin_target_encoding_custom_bin'])[f'{target_encode}'].values\n",
    "        \n",
    "        for param in self.parameters['categorical_mean_encoding']:\n",
    "            col = param\n",
    "            data_dummy = eval(f'self.{col}_categorical_mean_encoding')\n",
    "            data[f'{col}_categorical_mean_encoding'] = data[col].copy().values\n",
    "            data[f'{col}_categorical_mean_encoding'] = pd.merge(data,data_dummy,how='left',on=[f'{col}_categorical_mean_encoding'])[f'{target_encode}'].values\n",
    "        \n",
    "        \n",
    "        for cols in self.parameters['multiply']:\n",
    "            data[cols[0] + 'x' +cols[1]] = (data[cols[0]] * data[cols[1]]).values\n",
    "        for cols in self.parameters['add']:\n",
    "            data[cols[0] + '+' +cols[1]] = (data[cols[0]] + data[cols[1]]).values\n",
    "        for cols in self.parameters['add_str']:\n",
    "            data[cols[0] + '+' +cols[1]] = (data[cols[0]].astype(str)+'_' + data[cols[1]].astype(str)).values\n",
    "            \n",
    "        for cols in self.parameters['substract']:\n",
    "            data[cols[0] + '-' +cols[1]] = (data[cols[0]] - data[cols[1]]).values\n",
    "        for cols in self.parameters['divide']:\n",
    "            data[cols[0] + '/' +cols[1]] = (data[cols[0]] / np.where(data[cols[1]]==0,0.0001,data[cols[1]])).values\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        return data\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T14:57:37.737438Z",
     "start_time": "2021-02-15T14:57:37.033465Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 11)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_skf = {'train':[],'val':[]}\n",
    "skf = StratifiedKFold(n_splits=2,random_state = 3,shuffle = True)\n",
    "parameters = {'multiply':[['GPA','number_of_dependences']],\n",
    "              'add':[['annual_leave','sick_leaves'],['assign_of_otherposition','branch_rotation']],\n",
    "              'add_str':[['Education_level','job_level']],\n",
    "              'substract':[],'divide':[],\n",
    "              'bin_numer_qcut':[],\n",
    "              'bin_numer_cut':[['GPA',30]],\n",
    "              'bin_add_categ_numer_bin_qcut':[['job_level','GPA',5],['Education_level','GPA',5]],\n",
    "            'bin_target_encoding_cut':[],\n",
    "             'bin_target_encoding_qcut':[['year_graduated',5],['GPA',5],['annual_leave',5]],\n",
    "             'bin_target_encoding_custom_bin':[],\n",
    "              'categorical_mean_encoding':['job_level','person_level','Employee_type','Education_level'],\n",
    "             'target':'Best Performance'}\n",
    "\n",
    "\n",
    "for train_index,val_index in skf.split(X_train,y_train):\n",
    "    add_fe = Feature_Engineering(parameters)\n",
    "    add_fe.fit(data.iloc[train_index,:])\n",
    "    data_skf['train'].append([add_fe.transform(X.iloc[train_index,:],mode='val'),y.iloc[train_index]])\n",
    "    data_skf['val'].append([add_fe.transform(X.iloc[val_index,:],mode='val'),y.iloc[val_index]])\n",
    "\n",
    "num_cols_fe = list(data_skf['train'][0][0].select_dtypes(exclude='object').columns)\n",
    "cat_cols_fe = list(data_skf['train'][0][0].select_dtypes(include='object').columns)\n",
    "len(num_cols_fe),len(cat_cols_fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T15:18:23.784974Z",
     "start_time": "2021-02-15T15:17:23.393943Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | max_depth | n_esti... | num_le... | reg_alpha | reg_la... |\n",
      "-------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.5449  \u001b[0m | \u001b[0m 57.14   \u001b[0m | \u001b[0m 228.8   \u001b[0m | \u001b[0m 70.21   \u001b[0m | \u001b[0m 0.02724 \u001b[0m | \u001b[0m 0.02118 \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.5458  \u001b[0m | \u001b[95m 66.36   \u001b[0m | \u001b[95m 159.4   \u001b[0m | \u001b[95m 91.88   \u001b[0m | \u001b[95m 0.04818 \u001b[0m | \u001b[95m 0.01917 \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.5484  \u001b[0m | \u001b[95m 69.46   \u001b[0m | \u001b[95m 158.9   \u001b[0m | \u001b[95m 91.43   \u001b[0m | \u001b[95m 0.01489 \u001b[0m | \u001b[95m 0.01442 \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.5443  \u001b[0m | \u001b[0m 69.15   \u001b[0m | \u001b[0m 105.8   \u001b[0m | \u001b[0m 80.83   \u001b[0m | \u001b[0m 0.02445 \u001b[0m | \u001b[0m 0.0194  \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 0.5595  \u001b[0m | \u001b[95m 8.968   \u001b[0m | \u001b[95m 54.52   \u001b[0m | \u001b[95m 32.94   \u001b[0m | \u001b[95m 0.00111 \u001b[0m | \u001b[95m 0.003873\u001b[0m |\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.5486  \u001b[0m | \u001b[0m 7.353   \u001b[0m | \u001b[0m 61.64   \u001b[0m | \u001b[0m 31.64   \u001b[0m | \u001b[0m 0.01012 \u001b[0m | \u001b[0m 0.04933 \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.5592  \u001b[0m | \u001b[0m 11.94   \u001b[0m | \u001b[0m 55.12   \u001b[0m | \u001b[0m 30.38   \u001b[0m | \u001b[0m 0.02362 \u001b[0m | \u001b[0m 0.04494 \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.5506  \u001b[0m | \u001b[0m 14.02   \u001b[0m | \u001b[0m 51.63   \u001b[0m | \u001b[0m 37.0    \u001b[0m | \u001b[0m 0.006814\u001b[0m | \u001b[0m 0.006885\u001b[0m |\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m 0.5631  \u001b[0m | \u001b[95m 11.99   \u001b[0m | \u001b[95m 51.02   \u001b[0m | \u001b[95m 26.8    \u001b[0m | \u001b[95m 0.04022 \u001b[0m | \u001b[95m 0.03835 \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.5566  \u001b[0m | \u001b[0m 8.95    \u001b[0m | \u001b[0m 50.92   \u001b[0m | \u001b[0m 26.85   \u001b[0m | \u001b[0m 0.01833 \u001b[0m | \u001b[0m 0.009315\u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.5554  \u001b[0m | \u001b[0m 21.38   \u001b[0m | \u001b[0m 54.81   \u001b[0m | \u001b[0m 27.52   \u001b[0m | \u001b[0m 0.01597 \u001b[0m | \u001b[0m 0.03865 \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.5554  \u001b[0m | \u001b[0m 15.77   \u001b[0m | \u001b[0m 50.31   \u001b[0m | \u001b[0m 28.33   \u001b[0m | \u001b[0m 0.04535 \u001b[0m | \u001b[0m 0.02556 \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.5529  \u001b[0m | \u001b[0m 14.36   \u001b[0m | \u001b[0m 54.32   \u001b[0m | \u001b[0m 25.97   \u001b[0m | \u001b[0m 0.01127 \u001b[0m | \u001b[0m 0.0416  \u001b[0m |\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.5574  \u001b[0m | \u001b[0m 6.19    \u001b[0m | \u001b[0m 54.15   \u001b[0m | \u001b[0m 31.06   \u001b[0m | \u001b[0m 0.04916 \u001b[0m | \u001b[0m 0.03591 \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.5522  \u001b[0m | \u001b[0m 9.463   \u001b[0m | \u001b[0m 51.3    \u001b[0m | \u001b[0m 33.61   \u001b[0m | \u001b[0m 0.03051 \u001b[0m | \u001b[0m 0.02831 \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.5518  \u001b[0m | \u001b[0m 26.18   \u001b[0m | \u001b[0m 56.64   \u001b[0m | \u001b[0m 27.93   \u001b[0m | \u001b[0m 0.03555 \u001b[0m | \u001b[0m 0.04297 \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.5569  \u001b[0m | \u001b[0m 9.115   \u001b[0m | \u001b[0m 56.94   \u001b[0m | \u001b[0m 26.98   \u001b[0m | \u001b[0m 0.0427  \u001b[0m | \u001b[0m 0.02178 \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.5591  \u001b[0m | \u001b[0m 9.992   \u001b[0m | \u001b[0m 54.22   \u001b[0m | \u001b[0m 26.46   \u001b[0m | \u001b[0m 0.025   \u001b[0m | \u001b[0m 0.03284 \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.5579  \u001b[0m | \u001b[0m 5.548   \u001b[0m | \u001b[0m 55.71   \u001b[0m | \u001b[0m 34.59   \u001b[0m | \u001b[0m 0.01818 \u001b[0m | \u001b[0m 0.02755 \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.5529  \u001b[0m | \u001b[0m 7.019   \u001b[0m | \u001b[0m 55.17   \u001b[0m | \u001b[0m 43.52   \u001b[0m | \u001b[0m 0.04663 \u001b[0m | \u001b[0m 0.006346\u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.5412  \u001b[0m | \u001b[0m 27.11   \u001b[0m | \u001b[0m 152.8   \u001b[0m | \u001b[0m 35.03   \u001b[0m | \u001b[0m 0.04231 \u001b[0m | \u001b[0m 0.01208 \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.544   \u001b[0m | \u001b[0m 26.74   \u001b[0m | \u001b[0m 299.7   \u001b[0m | \u001b[0m 63.96   \u001b[0m | \u001b[0m 0.03959 \u001b[0m | \u001b[0m 0.01871 \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.5608  \u001b[0m | \u001b[0m 5.641   \u001b[0m | \u001b[0m 63.05   \u001b[0m | \u001b[0m 57.72   \u001b[0m | \u001b[0m 0.03376 \u001b[0m | \u001b[0m 0.006835\u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.5611  \u001b[0m | \u001b[0m 5.392   \u001b[0m | \u001b[0m 64.85   \u001b[0m | \u001b[0m 53.66   \u001b[0m | \u001b[0m 0.01345 \u001b[0m | \u001b[0m 0.02669 \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.5592  \u001b[0m | \u001b[0m 5.101   \u001b[0m | \u001b[0m 69.2    \u001b[0m | \u001b[0m 58.89   \u001b[0m | \u001b[0m 0.00142 \u001b[0m | \u001b[0m 0.03926 \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.5497  \u001b[0m | \u001b[0m 10.5    \u001b[0m | \u001b[0m 66.6    \u001b[0m | \u001b[0m 54.51   \u001b[0m | \u001b[0m 0.03217 \u001b[0m | \u001b[0m 0.02011 \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.5543  \u001b[0m | \u001b[0m 7.487   \u001b[0m | \u001b[0m 56.8    \u001b[0m | \u001b[0m 55.23   \u001b[0m | \u001b[0m 0.01187 \u001b[0m | \u001b[0m 0.02875 \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.5543  \u001b[0m | \u001b[0m 6.285   \u001b[0m | \u001b[0m 61.23   \u001b[0m | \u001b[0m 64.25   \u001b[0m | \u001b[0m 0.001518\u001b[0m | \u001b[0m 0.01292 \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.5565  \u001b[0m | \u001b[0m 6.668   \u001b[0m | \u001b[0m 72.59   \u001b[0m | \u001b[0m 62.5    \u001b[0m | \u001b[0m 0.04852 \u001b[0m | \u001b[0m 0.0331  \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.5592  \u001b[0m | \u001b[0m 5.221   \u001b[0m | \u001b[0m 66.94   \u001b[0m | \u001b[0m 55.91   \u001b[0m | \u001b[0m 0.002207\u001b[0m | \u001b[0m 0.01335 \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m 0.5565  \u001b[0m | \u001b[0m 5.086   \u001b[0m | \u001b[0m 62.98   \u001b[0m | \u001b[0m 45.78   \u001b[0m | \u001b[0m 0.01722 \u001b[0m | \u001b[0m 0.04872 \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m 0.5588  \u001b[0m | \u001b[0m 5.305   \u001b[0m | \u001b[0m 72.43   \u001b[0m | \u001b[0m 54.52   \u001b[0m | \u001b[0m 0.02448 \u001b[0m | \u001b[0m 0.02133 \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m 0.5586  \u001b[0m | \u001b[0m 6.606   \u001b[0m | \u001b[0m 75.24   \u001b[0m | \u001b[0m 48.92   \u001b[0m | \u001b[0m 0.03154 \u001b[0m | \u001b[0m 0.01038 \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m 0.5568  \u001b[0m | \u001b[0m 6.145   \u001b[0m | \u001b[0m 68.0    \u001b[0m | \u001b[0m 46.63   \u001b[0m | \u001b[0m 0.01513 \u001b[0m | \u001b[0m 0.04986 \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m 0.5508  \u001b[0m | \u001b[0m 8.406   \u001b[0m | \u001b[0m 81.07   \u001b[0m | \u001b[0m 51.78   \u001b[0m | \u001b[0m 0.03844 \u001b[0m | \u001b[0m 0.002681\u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m 0.5434  \u001b[0m | \u001b[0m 9.064   \u001b[0m | \u001b[0m 73.82   \u001b[0m | \u001b[0m 41.75   \u001b[0m | \u001b[0m 0.04291 \u001b[0m | \u001b[0m 0.04204 \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m 0.557   \u001b[0m | \u001b[0m 5.687   \u001b[0m | \u001b[0m 68.46   \u001b[0m | \u001b[0m 66.9    \u001b[0m | \u001b[0m 0.03659 \u001b[0m | \u001b[0m 0.01786 \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m 0.5546  \u001b[0m | \u001b[0m 12.06   \u001b[0m | \u001b[0m 69.79   \u001b[0m | \u001b[0m 70.59   \u001b[0m | \u001b[0m 0.02926 \u001b[0m | \u001b[0m 0.03131 \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m 0.5482  \u001b[0m | \u001b[0m 7.131   \u001b[0m | \u001b[0m 71.02   \u001b[0m | \u001b[0m 50.47   \u001b[0m | \u001b[0m 0.02719 \u001b[0m | \u001b[0m 0.02016 \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m 0.5591  \u001b[0m | \u001b[0m 5.027   \u001b[0m | \u001b[0m 72.42   \u001b[0m | \u001b[0m 70.36   \u001b[0m | \u001b[0m 0.01051 \u001b[0m | \u001b[0m 0.01856 \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m 0.5534  \u001b[0m | \u001b[0m 6.263   \u001b[0m | \u001b[0m 77.47   \u001b[0m | \u001b[0m 69.25   \u001b[0m | \u001b[0m 0.04914 \u001b[0m | \u001b[0m 0.03389 \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m 0.5605  \u001b[0m | \u001b[0m 5.541   \u001b[0m | \u001b[0m 68.09   \u001b[0m | \u001b[0m 70.9    \u001b[0m | \u001b[0m 0.02924 \u001b[0m | \u001b[0m 0.02189 \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m 0.5475  \u001b[0m | \u001b[0m 8.128   \u001b[0m | \u001b[0m 64.9    \u001b[0m | \u001b[0m 74.68   \u001b[0m | \u001b[0m 0.04031 \u001b[0m | \u001b[0m 0.04582 \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m 0.5515  \u001b[0m | \u001b[0m 7.853   \u001b[0m | \u001b[0m 73.4    \u001b[0m | \u001b[0m 73.79   \u001b[0m | \u001b[0m 0.04199 \u001b[0m | \u001b[0m 0.03613 \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m 0.5608  \u001b[0m | \u001b[0m 5.577   \u001b[0m | \u001b[0m 61.9    \u001b[0m | \u001b[0m 50.66   \u001b[0m | \u001b[0m 0.04936 \u001b[0m | \u001b[0m 0.01592 \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m 0.5504  \u001b[0m | \u001b[0m 8.964   \u001b[0m | \u001b[0m 65.68   \u001b[0m | \u001b[0m 62.34   \u001b[0m | \u001b[0m 0.01104 \u001b[0m | \u001b[0m 0.01316 \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m 0.5584  \u001b[0m | \u001b[0m 5.984   \u001b[0m | \u001b[0m 60.33   \u001b[0m | \u001b[0m 55.36   \u001b[0m | \u001b[0m 0.01857 \u001b[0m | \u001b[0m 0.02955 \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m 0.5552  \u001b[0m | \u001b[0m 7.544   \u001b[0m | \u001b[0m 77.44   \u001b[0m | \u001b[0m 56.4    \u001b[0m | \u001b[0m 0.01237 \u001b[0m | \u001b[0m 0.01159 \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m 0.5599  \u001b[0m | \u001b[0m 10.83   \u001b[0m | \u001b[0m 61.99   \u001b[0m | \u001b[0m 48.98   \u001b[0m | \u001b[0m 0.03519 \u001b[0m | \u001b[0m 0.02741 \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m 0.5515  \u001b[0m | \u001b[0m 11.61   \u001b[0m | \u001b[0m 51.65   \u001b[0m | \u001b[0m 29.13   \u001b[0m | \u001b[0m 0.02162 \u001b[0m | \u001b[0m 0.04722 \u001b[0m |\n",
      "| \u001b[0m 51      \u001b[0m | \u001b[0m 0.555   \u001b[0m | \u001b[0m 9.066   \u001b[0m | \u001b[0m 59.17   \u001b[0m | \u001b[0m 50.88   \u001b[0m | \u001b[0m 0.03266 \u001b[0m | \u001b[0m 0.02474 \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m 0.5546  \u001b[0m | \u001b[0m 8.038   \u001b[0m | \u001b[0m 63.4    \u001b[0m | \u001b[0m 48.37   \u001b[0m | \u001b[0m 0.007863\u001b[0m | \u001b[0m 0.03124 \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m 0.5583  \u001b[0m | \u001b[0m 6.015   \u001b[0m | \u001b[0m 71.71   \u001b[0m | \u001b[0m 56.98   \u001b[0m | \u001b[0m 0.04898 \u001b[0m | \u001b[0m 0.04334 \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 54      \u001b[0m | \u001b[0m 0.55    \u001b[0m | \u001b[0m 13.6    \u001b[0m | \u001b[0m 61.37   \u001b[0m | \u001b[0m 47.82   \u001b[0m | \u001b[0m 0.03991 \u001b[0m | \u001b[0m 0.02403 \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m 0.5614  \u001b[0m | \u001b[0m 5.531   \u001b[0m | \u001b[0m 64.86   \u001b[0m | \u001b[0m 57.58   \u001b[0m | \u001b[0m 0.02859 \u001b[0m | \u001b[0m 0.008894\u001b[0m |\n",
      "| \u001b[0m 56      \u001b[0m | \u001b[0m 0.5525  \u001b[0m | \u001b[0m 8.514   \u001b[0m | \u001b[0m 63.77   \u001b[0m | \u001b[0m 53.7    \u001b[0m | \u001b[0m 0.0383  \u001b[0m | \u001b[0m 0.004549\u001b[0m |\n",
      "| \u001b[0m 57      \u001b[0m | \u001b[0m 0.5548  \u001b[0m | \u001b[0m 13.31   \u001b[0m | \u001b[0m 57.21   \u001b[0m | \u001b[0m 32.35   \u001b[0m | \u001b[0m 0.02857 \u001b[0m | \u001b[0m 0.03842 \u001b[0m |\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "| \u001b[0m 58      \u001b[0m | \u001b[0m 0.5525  \u001b[0m | \u001b[0m 7.629   \u001b[0m | \u001b[0m 56.49   \u001b[0m | \u001b[0m 31.15   \u001b[0m | \u001b[0m 0.04611 \u001b[0m | \u001b[0m 0.01844 \u001b[0m |\n",
      "| \u001b[0m 59      \u001b[0m | \u001b[0m 0.5455  \u001b[0m | \u001b[0m 8.347   \u001b[0m | \u001b[0m 68.84   \u001b[0m | \u001b[0m 68.05   \u001b[0m | \u001b[0m 0.001883\u001b[0m | \u001b[0m 0.03472 \u001b[0m |\n",
      "| \u001b[0m 60      \u001b[0m | \u001b[0m 0.5563  \u001b[0m | \u001b[0m 7.225   \u001b[0m | \u001b[0m 62.41   \u001b[0m | \u001b[0m 56.12   \u001b[0m | \u001b[0m 0.02285 \u001b[0m | \u001b[0m 0.03575 \u001b[0m |\n",
      "| \u001b[0m 61      \u001b[0m | \u001b[0m 0.5601  \u001b[0m | \u001b[0m 14.29   \u001b[0m | \u001b[0m 50.99   \u001b[0m | \u001b[0m 25.63   \u001b[0m | \u001b[0m 0.03194 \u001b[0m | \u001b[0m 0.03401 \u001b[0m |\n",
      "| \u001b[0m 62      \u001b[0m | \u001b[0m 0.5549  \u001b[0m | \u001b[0m 5.006   \u001b[0m | \u001b[0m 75.73   \u001b[0m | \u001b[0m 58.34   \u001b[0m | \u001b[0m 0.01617 \u001b[0m | \u001b[0m 0.02545 \u001b[0m |\n",
      "| \u001b[0m 63      \u001b[0m | \u001b[0m 0.5627  \u001b[0m | \u001b[0m 5.066   \u001b[0m | \u001b[0m 60.48   \u001b[0m | \u001b[0m 49.43   \u001b[0m | \u001b[0m 0.02152 \u001b[0m | \u001b[0m 0.02092 \u001b[0m |\n",
      "| \u001b[0m 64      \u001b[0m | \u001b[0m 0.5625  \u001b[0m | \u001b[0m 5.399   \u001b[0m | \u001b[0m 66.42   \u001b[0m | \u001b[0m 51.21   \u001b[0m | \u001b[0m 0.004405\u001b[0m | \u001b[0m 0.02366 \u001b[0m |\n",
      "| \u001b[0m 65      \u001b[0m | \u001b[0m 0.5572  \u001b[0m | \u001b[0m 6.704   \u001b[0m | \u001b[0m 60.27   \u001b[0m | \u001b[0m 48.52   \u001b[0m | \u001b[0m 0.02085 \u001b[0m | \u001b[0m 0.008148\u001b[0m |\n",
      "| \u001b[0m 66      \u001b[0m | \u001b[0m 0.5605  \u001b[0m | \u001b[0m 5.096   \u001b[0m | \u001b[0m 64.35   \u001b[0m | \u001b[0m 48.1    \u001b[0m | \u001b[0m 0.02531 \u001b[0m | \u001b[0m 0.0146  \u001b[0m |\n",
      "| \u001b[0m 67      \u001b[0m | \u001b[0m 0.5588  \u001b[0m | \u001b[0m 6.526   \u001b[0m | \u001b[0m 66.75   \u001b[0m | \u001b[0m 49.26   \u001b[0m | \u001b[0m 0.04093 \u001b[0m | \u001b[0m 0.03419 \u001b[0m |\n",
      "| \u001b[0m 68      \u001b[0m | \u001b[0m 0.5533  \u001b[0m | \u001b[0m 6.131   \u001b[0m | \u001b[0m 58.15   \u001b[0m | \u001b[0m 50.42   \u001b[0m | \u001b[0m 0.0337  \u001b[0m | \u001b[0m 0.04173 \u001b[0m |\n",
      "| \u001b[0m 69      \u001b[0m | \u001b[0m 0.5559  \u001b[0m | \u001b[0m 6.397   \u001b[0m | \u001b[0m 54.01   \u001b[0m | \u001b[0m 37.39   \u001b[0m | \u001b[0m 0.02378 \u001b[0m | \u001b[0m 0.02864 \u001b[0m |\n",
      "| \u001b[0m 70      \u001b[0m | \u001b[0m 0.5626  \u001b[0m | \u001b[0m 5.1     \u001b[0m | \u001b[0m 62.33   \u001b[0m | \u001b[0m 59.8    \u001b[0m | \u001b[0m 0.01941 \u001b[0m | \u001b[0m 0.006245\u001b[0m |\n",
      "| \u001b[0m 71      \u001b[0m | \u001b[0m 0.5617  \u001b[0m | \u001b[0m 5.588   \u001b[0m | \u001b[0m 54.4    \u001b[0m | \u001b[0m 60.36   \u001b[0m | \u001b[0m 0.009543\u001b[0m | \u001b[0m 0.04114 \u001b[0m |\n",
      "| \u001b[0m 72      \u001b[0m | \u001b[0m 0.5499  \u001b[0m | \u001b[0m 8.646   \u001b[0m | \u001b[0m 54.54   \u001b[0m | \u001b[0m 62.29   \u001b[0m | \u001b[0m 0.04165 \u001b[0m | \u001b[0m 0.04534 \u001b[0m |\n",
      "| \u001b[0m 73      \u001b[0m | \u001b[0m 0.5582  \u001b[0m | \u001b[0m 5.13    \u001b[0m | \u001b[0m 61.18   \u001b[0m | \u001b[0m 53.02   \u001b[0m | \u001b[0m 0.03236 \u001b[0m | \u001b[0m 0.04633 \u001b[0m |\n",
      "| \u001b[0m 74      \u001b[0m | \u001b[0m 0.5601  \u001b[0m | \u001b[0m 5.055   \u001b[0m | \u001b[0m 70.66   \u001b[0m | \u001b[0m 73.2    \u001b[0m | \u001b[0m 0.04275 \u001b[0m | \u001b[0m 0.0468  \u001b[0m |\n",
      "| \u001b[0m 75      \u001b[0m | \u001b[0m 0.5621  \u001b[0m | \u001b[0m 6.458   \u001b[0m | \u001b[0m 52.58   \u001b[0m | \u001b[0m 56.72   \u001b[0m | \u001b[0m 0.03212 \u001b[0m | \u001b[0m 0.03981 \u001b[0m |\n",
      "| \u001b[0m 76      \u001b[0m | \u001b[0m 0.5596  \u001b[0m | \u001b[0m 11.94   \u001b[0m | \u001b[0m 50.87   \u001b[0m | \u001b[0m 25.54   \u001b[0m | \u001b[0m 0.007071\u001b[0m | \u001b[0m 0.03413 \u001b[0m |\n",
      "| \u001b[0m 77      \u001b[0m | \u001b[0m 0.5631  \u001b[0m | \u001b[0m 5.153   \u001b[0m | \u001b[0m 50.14   \u001b[0m | \u001b[0m 60.44   \u001b[0m | \u001b[0m 0.03372 \u001b[0m | \u001b[0m 0.02284 \u001b[0m |\n",
      "| \u001b[0m 78      \u001b[0m | \u001b[0m 0.5596  \u001b[0m | \u001b[0m 5.015   \u001b[0m | \u001b[0m 52.02   \u001b[0m | \u001b[0m 52.82   \u001b[0m | \u001b[0m 0.02566 \u001b[0m | \u001b[0m 0.02789 \u001b[0m |\n",
      "| \u001b[0m 79      \u001b[0m | \u001b[0m 0.5601  \u001b[0m | \u001b[0m 6.357   \u001b[0m | \u001b[0m 51.06   \u001b[0m | \u001b[0m 61.58   \u001b[0m | \u001b[0m 0.001866\u001b[0m | \u001b[0m 0.001911\u001b[0m |\n",
      "| \u001b[0m 80      \u001b[0m | \u001b[0m 0.5526  \u001b[0m | \u001b[0m 9.355   \u001b[0m | \u001b[0m 50.04   \u001b[0m | \u001b[0m 57.5    \u001b[0m | \u001b[0m 0.03151 \u001b[0m | \u001b[0m 0.009546\u001b[0m |\n",
      "| \u001b[0m 81      \u001b[0m | \u001b[0m 0.5598  \u001b[0m | \u001b[0m 6.045   \u001b[0m | \u001b[0m 50.52   \u001b[0m | \u001b[0m 47.54   \u001b[0m | \u001b[0m 0.03276 \u001b[0m | \u001b[0m 0.04556 \u001b[0m |\n",
      "| \u001b[0m 82      \u001b[0m | \u001b[0m 0.5513  \u001b[0m | \u001b[0m 18.34   \u001b[0m | \u001b[0m 73.4    \u001b[0m | \u001b[0m 69.93   \u001b[0m | \u001b[0m 0.02722 \u001b[0m | \u001b[0m 0.003797\u001b[0m |\n",
      "| \u001b[0m 83      \u001b[0m | \u001b[0m 0.552   \u001b[0m | \u001b[0m 9.81    \u001b[0m | \u001b[0m 51.58   \u001b[0m | \u001b[0m 49.82   \u001b[0m | \u001b[0m 0.004077\u001b[0m | \u001b[0m 0.04632 \u001b[0m |\n",
      "| \u001b[0m 84      \u001b[0m | \u001b[0m 0.563   \u001b[0m | \u001b[0m 5.393   \u001b[0m | \u001b[0m 52.61   \u001b[0m | \u001b[0m 48.93   \u001b[0m | \u001b[0m 0.000578\u001b[0m | \u001b[0m 0.03446 \u001b[0m |\n",
      "| \u001b[95m 85      \u001b[0m | \u001b[95m 0.5631  \u001b[0m | \u001b[95m 5.607   \u001b[0m | \u001b[95m 51.92   \u001b[0m | \u001b[95m 45.45   \u001b[0m | \u001b[95m 0.03717 \u001b[0m | \u001b[95m 0.005547\u001b[0m |\n",
      "| \u001b[0m 86      \u001b[0m | \u001b[0m 0.5429  \u001b[0m | \u001b[0m 29.72   \u001b[0m | \u001b[0m 195.7   \u001b[0m | \u001b[0m 74.58   \u001b[0m | \u001b[0m 0.04851 \u001b[0m | \u001b[0m 0.04566 \u001b[0m |\n",
      "| \u001b[0m 87      \u001b[0m | \u001b[0m 0.5521  \u001b[0m | \u001b[0m 6.794   \u001b[0m | \u001b[0m 55.67   \u001b[0m | \u001b[0m 57.85   \u001b[0m | \u001b[0m 0.0223  \u001b[0m | \u001b[0m 0.04331 \u001b[0m |\n",
      "| \u001b[0m 88      \u001b[0m | \u001b[0m 0.5559  \u001b[0m | \u001b[0m 5.356   \u001b[0m | \u001b[0m 76.05   \u001b[0m | \u001b[0m 51.7    \u001b[0m | \u001b[0m 0.01374 \u001b[0m | \u001b[0m 0.02513 \u001b[0m |\n",
      "| \u001b[0m 89      \u001b[0m | \u001b[0m 0.559   \u001b[0m | \u001b[0m 6.169   \u001b[0m | \u001b[0m 54.67   \u001b[0m | \u001b[0m 25.2    \u001b[0m | \u001b[0m 0.03532 \u001b[0m | \u001b[0m 0.009712\u001b[0m |\n",
      "| \u001b[0m 90      \u001b[0m | \u001b[0m 0.545   \u001b[0m | \u001b[0m 80.78   \u001b[0m | \u001b[0m 115.5   \u001b[0m | \u001b[0m 39.68   \u001b[0m | \u001b[0m 0.04233 \u001b[0m | \u001b[0m 0.01551 \u001b[0m |\n",
      "| \u001b[95m 91      \u001b[0m | \u001b[95m 0.5636  \u001b[0m | \u001b[95m 5.387   \u001b[0m | \u001b[95m 50.73   \u001b[0m | \u001b[95m 57.98   \u001b[0m | \u001b[95m 0.03588 \u001b[0m | \u001b[95m 0.003992\u001b[0m |\n",
      "| \u001b[0m 92      \u001b[0m | \u001b[0m 0.5407  \u001b[0m | \u001b[0m 29.5    \u001b[0m | \u001b[0m 230.3   \u001b[0m | \u001b[0m 34.18   \u001b[0m | \u001b[0m 0.04393 \u001b[0m | \u001b[0m 0.02259 \u001b[0m |\n",
      "| \u001b[95m 93      \u001b[0m | \u001b[95m 0.5641  \u001b[0m | \u001b[95m 5.664   \u001b[0m | \u001b[95m 52.3    \u001b[0m | \u001b[95m 60.8    \u001b[0m | \u001b[95m 0.007086\u001b[0m | \u001b[95m 0.04599 \u001b[0m |\n",
      "| \u001b[0m 94      \u001b[0m | \u001b[0m 0.5526  \u001b[0m | \u001b[0m 7.092   \u001b[0m | \u001b[0m 54.23   \u001b[0m | \u001b[0m 47.54   \u001b[0m | \u001b[0m 0.003814\u001b[0m | \u001b[0m 0.007481\u001b[0m |\n",
      "| \u001b[0m 95      \u001b[0m | \u001b[0m 0.5634  \u001b[0m | \u001b[0m 5.257   \u001b[0m | \u001b[0m 52.77   \u001b[0m | \u001b[0m 64.75   \u001b[0m | \u001b[0m 0.005235\u001b[0m | \u001b[0m 0.04046 \u001b[0m |\n",
      "| \u001b[0m 96      \u001b[0m | \u001b[0m 0.5577  \u001b[0m | \u001b[0m 5.616   \u001b[0m | \u001b[0m 53.13   \u001b[0m | \u001b[0m 67.1    \u001b[0m | \u001b[0m 0.03023 \u001b[0m | \u001b[0m 0.0437  \u001b[0m |\n",
      "| \u001b[0m 97      \u001b[0m | \u001b[0m 0.5477  \u001b[0m | \u001b[0m 95.02   \u001b[0m | \u001b[0m 263.7   \u001b[0m | \u001b[0m 25.84   \u001b[0m | \u001b[0m 0.00231 \u001b[0m | \u001b[0m 0.03581 \u001b[0m |\n",
      "| \u001b[0m 98      \u001b[0m | \u001b[0m 0.5413  \u001b[0m | \u001b[0m 95.12   \u001b[0m | \u001b[0m 297.3   \u001b[0m | \u001b[0m 63.27   \u001b[0m | \u001b[0m 0.04069 \u001b[0m | \u001b[0m 0.003109\u001b[0m |\n",
      "| \u001b[0m 99      \u001b[0m | \u001b[0m 0.5601  \u001b[0m | \u001b[0m 5.19    \u001b[0m | \u001b[0m 50.41   \u001b[0m | \u001b[0m 41.95   \u001b[0m | \u001b[0m 0.001375\u001b[0m | \u001b[0m 0.01553 \u001b[0m |\n",
      "| \u001b[0m 100     \u001b[0m | \u001b[0m 0.5552  \u001b[0m | \u001b[0m 89.36   \u001b[0m | \u001b[0m 50.59   \u001b[0m | \u001b[0m 43.54   \u001b[0m | \u001b[0m 0.01553 \u001b[0m | \u001b[0m 0.02172 \u001b[0m |\n",
      "| \u001b[0m 101     \u001b[0m | \u001b[0m 0.5528  \u001b[0m | \u001b[0m 80.43   \u001b[0m | \u001b[0m 57.07   \u001b[0m | \u001b[0m 40.83   \u001b[0m | \u001b[0m 0.04583 \u001b[0m | \u001b[0m 0.04657 \u001b[0m |\n",
      "| \u001b[0m 102     \u001b[0m | \u001b[0m 0.5489  \u001b[0m | \u001b[0m 96.57   \u001b[0m | \u001b[0m 52.55   \u001b[0m | \u001b[0m 50.28   \u001b[0m | \u001b[0m 0.0112  \u001b[0m | \u001b[0m 0.04832 \u001b[0m |\n",
      "=====================================================================================\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "num_transformer = Pipeline(steps=[\n",
    "                                ('imputer', SimpleImputer(strategy = 'median')),\n",
    "                                ('scaler', RobustScaler())\n",
    "                                ])\n",
    "\n",
    "cat_transformer = Pipeline(steps=[\n",
    "                                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                                ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "                                ])\n",
    "    \n",
    "def search_model_lgbm(num_leaves,max_depth,n_estimators,reg_alpha,reg_lambda):\n",
    "    params = {\n",
    "        'boosting_type':'gbdt',\n",
    "         'num_leaves':int(num_leaves),\n",
    "         'max_depth':int(max_depth),\n",
    "         'n_estimators':int(n_estimators),\n",
    "         'objective':'binary',\n",
    "         'class_weight':'balanced',\n",
    "         'reg_alpha':reg_alpha,\n",
    "         'reg_lambda':reg_lambda,\n",
    "         'random_state':0}\n",
    "    lgbm = LGBMClassifier(**params)\n",
    "    transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', num_transformer, num_cols_fe),\n",
    "            ('cat', cat_transformer, cat_cols_fe)\n",
    "        ])\n",
    "    main_pipeline = Pipeline(steps=[('transformer', transformer),\n",
    "                      ('classifier', lgbm)])\n",
    "    \n",
    "    aucs = []\n",
    "    for i in range(len(data_skf['train'])):\n",
    "        model = clone(main_pipeline)\n",
    "        model.fit(data_skf['train'][i][0],data_skf['train'][i][1].values)\n",
    "        pred_proba = model.predict_proba(data_skf['val'][i][0])[:,1]\n",
    "        \n",
    "        aucs.append(roc_auc_score(data_skf['val'][i][1].values, pred_proba,average='weighted'))\n",
    "    return np.mean(aucs)\n",
    "\n",
    "lgbBO = BayesianOptimization(search_model_lgbm, {'num_leaves': (25, 100),\n",
    "                                        'max_depth': (5, 100),\n",
    "                                        'n_estimators':(50,300),\n",
    "                                        'reg_alpha': (0.0, 0.05),\n",
    "                                        'reg_lambda': (0.0, 0.05),\n",
    "                                        },random_state=0)\n",
    "\n",
    "lgbBO.maximize(n_iter=100, init_points=2)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T16:01:00.336755Z",
     "start_time": "2021-02-15T16:01:00.324728Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': 0.5641272772549423,\n",
       " 'params': {'max_depth': 5.663953027572844,\n",
       "  'n_estimators': 52.29955231632742,\n",
       "  'num_leaves': 60.80441469032743,\n",
       "  'reg_alpha': 0.0070860708219850025,\n",
       "  'reg_lambda': 0.04598746020415847}}"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbBO.max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T16:01:01.765828Z",
     "start_time": "2021-02-15T16:01:01.747798Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type':'gbdt',\n",
    "     'num_leaves':int(60.80441469032743),\n",
    "     'max_depth':int(5.663953027572844),\n",
    "     'n_estimators':int(52.29955231632742),\n",
    "     'objective':'binary',\n",
    "     'class_weight':'balanced',\n",
    "     'reg_alpha':0.0070860708219850025,\n",
    "     'reg_lambda':0.04598746020415847,\n",
    "     'random_state':0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T16:03:14.558613Z",
     "start_time": "2021-02-15T16:03:14.099660Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5641272772549423"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "lgbm = LGBMClassifier(**params)\n",
    "transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_cols_fe),\n",
    "        ('cat', cat_transformer, cat_cols_fe)\n",
    "    ])\n",
    "main_pipeline = Pipeline(steps=[('transformer', transformer),\n",
    "                  ('classifier', lgbm)])\n",
    "\n",
    "aucs = []\n",
    "for i in range(len(data_skf['train'])):\n",
    "    model = clone(main_pipeline)\n",
    "    model.fit(data_skf['train'][i][0],data_skf['train'][i][1].values)\n",
    "    pred_proba = model.predict_proba(data_skf['val'][i][0])[:,1]\n",
    "\n",
    "    aucs.append(roc_auc_score(data_skf['val'][i][1].values, pred_proba,average='weighted'))\n",
    "\n",
    "np.mean(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T16:05:13.423943Z",
     "start_time": "2021-02-15T16:05:12.460776Z"
    }
   },
   "outputs": [],
   "source": [
    "add_fe = Feature_Engineering(parameters)\n",
    "add_fe.fit(data.iloc[train_index,:])\n",
    "\n",
    "X_train, y_train = add_fe.transform(X,mode='val'),y.copy()\n",
    "X_test = add_fe.transform(data_test,mode='val')\n",
    "\n",
    "\n",
    "model = clone(main_pipeline)\n",
    "model.fit(X_train,y_train.values)\n",
    "\n",
    "pred_proba = model.predict_proba(X_test)[:,1]\n",
    "df_submission = pd.DataFrame({'index':data_test.index,'Best Performance':pred_proba})\n",
    "df_submission\n",
    "\n",
    "df_submission.to_csv('df_submission_15feb_LGBM1CVTUNE_FE.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T16:05:51.861372Z",
     "start_time": "2021-02-15T16:05:51.845342Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36966666666666664"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pred_proba[pred_proba>0.5])/len(pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T15:48:01.535247Z",
     "start_time": "2021-02-15T15:36:50.788244Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | max_depth | n_esti... | num_le... | reg_alpha | reg_la... | scale_... |\n",
      "-------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.5468  \u001b[0m | \u001b[0m 57.14   \u001b[0m | \u001b[0m 228.8   \u001b[0m | \u001b[0m 70.21   \u001b[0m | \u001b[0m 0.02724 \u001b[0m | \u001b[0m 0.02118 \u001b[0m | \u001b[0m 6.167   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.5476  \u001b[0m | \u001b[95m 46.57   \u001b[0m | \u001b[95m 272.9   \u001b[0m | \u001b[95m 97.27   \u001b[0m | \u001b[95m 0.01917 \u001b[0m | \u001b[95m 0.03959 \u001b[0m | \u001b[95m 5.231   \u001b[0m |\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.5519  \u001b[0m | \u001b[95m 51.74   \u001b[0m | \u001b[95m 273.7   \u001b[0m | \u001b[95m 97.71   \u001b[0m | \u001b[95m 0.01531 \u001b[0m | \u001b[95m 0.03104 \u001b[0m | \u001b[95m 2.652   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.5495  \u001b[0m | \u001b[0m 51.15   \u001b[0m | \u001b[0m 274.1   \u001b[0m | \u001b[0m 99.85   \u001b[0m | \u001b[0m 0.04614 \u001b[0m | \u001b[0m 0.001583\u001b[0m | \u001b[0m 4.256   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.5426  \u001b[0m | \u001b[0m 50.38   \u001b[0m | \u001b[0m 270.5   \u001b[0m | \u001b[0m 96.32   \u001b[0m | \u001b[0m 0.03668 \u001b[0m | \u001b[0m 0.02731 \u001b[0m | \u001b[0m 2.179   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.5504  \u001b[0m | \u001b[0m 49.64   \u001b[0m | \u001b[0m 275.3   \u001b[0m | \u001b[0m 95.06   \u001b[0m | \u001b[0m 0.03037 \u001b[0m | \u001b[0m 0.02953 \u001b[0m | \u001b[0m 4.64    \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.556   \u001b[0m | \u001b[95m 52.32   \u001b[0m | \u001b[95m 278.3   \u001b[0m | \u001b[95m 96.99   \u001b[0m | \u001b[95m 0.048   \u001b[0m | \u001b[95m 0.04721 \u001b[0m | \u001b[95m 3.802   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.5496  \u001b[0m | \u001b[0m 46.85   \u001b[0m | \u001b[0m 275.0   \u001b[0m | \u001b[0m 93.92   \u001b[0m | \u001b[0m 0.04643 \u001b[0m | \u001b[0m 0.01701 \u001b[0m | \u001b[0m 5.171   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.5546  \u001b[0m | \u001b[0m 52.88   \u001b[0m | \u001b[0m 277.9   \u001b[0m | \u001b[0m 96.35   \u001b[0m | \u001b[0m 0.02263 \u001b[0m | \u001b[0m 0.04395 \u001b[0m | \u001b[0m 2.37    \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.5501  \u001b[0m | \u001b[0m 56.81   \u001b[0m | \u001b[0m 276.2   \u001b[0m | \u001b[0m 98.04   \u001b[0m | \u001b[0m 0.01017 \u001b[0m | \u001b[0m 0.02746 \u001b[0m | \u001b[0m 3.179   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.5455  \u001b[0m | \u001b[0m 50.26   \u001b[0m | \u001b[0m 280.3   \u001b[0m | \u001b[0m 96.25   \u001b[0m | \u001b[0m 0.04538 \u001b[0m | \u001b[0m 0.04009 \u001b[0m | \u001b[0m 4.996   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.5466  \u001b[0m | \u001b[0m 52.07   \u001b[0m | \u001b[0m 280.8   \u001b[0m | \u001b[0m 97.42   \u001b[0m | \u001b[0m 0.0199  \u001b[0m | \u001b[0m 0.04794 \u001b[0m | \u001b[0m 2.011   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.5523  \u001b[0m | \u001b[0m 55.1    \u001b[0m | \u001b[0m 280.1   \u001b[0m | \u001b[0m 93.71   \u001b[0m | \u001b[0m 0.03581 \u001b[0m | \u001b[0m 0.01911 \u001b[0m | \u001b[0m 1.768   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.5428  \u001b[0m | \u001b[0m 57.04   \u001b[0m | \u001b[0m 279.4   \u001b[0m | \u001b[0m 94.86   \u001b[0m | \u001b[0m 0.03471 \u001b[0m | \u001b[0m 0.02561 \u001b[0m | \u001b[0m 2.741   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.5463  \u001b[0m | \u001b[0m 48.93   \u001b[0m | \u001b[0m 277.6   \u001b[0m | \u001b[0m 97.58   \u001b[0m | \u001b[0m 0.025   \u001b[0m | \u001b[0m 0.02352 \u001b[0m | \u001b[0m 2.935   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.5486  \u001b[0m | \u001b[0m 59.37   \u001b[0m | \u001b[0m 273.5   \u001b[0m | \u001b[0m 99.35   \u001b[0m | \u001b[0m 0.0334  \u001b[0m | \u001b[0m 0.006162\u001b[0m | \u001b[0m 5.591   \u001b[0m |\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.5507  \u001b[0m | \u001b[0m 53.94   \u001b[0m | \u001b[0m 279.8   \u001b[0m | \u001b[0m 97.02   \u001b[0m | \u001b[0m 0.04499 \u001b[0m | \u001b[0m 0.03052 \u001b[0m | \u001b[0m 3.175   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.5458  \u001b[0m | \u001b[0m 52.04   \u001b[0m | \u001b[0m 276.4   \u001b[0m | \u001b[0m 96.46   \u001b[0m | \u001b[0m 0.008279\u001b[0m | \u001b[0m 0.01292 \u001b[0m | \u001b[0m 5.944   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.5449  \u001b[0m | \u001b[0m 55.52   \u001b[0m | \u001b[0m 270.8   \u001b[0m | \u001b[0m 99.4    \u001b[0m | \u001b[0m 0.02612 \u001b[0m | \u001b[0m 0.04073 \u001b[0m | \u001b[0m 3.392   \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.5458  \u001b[0m | \u001b[0m 56.62   \u001b[0m | \u001b[0m 278.5   \u001b[0m | \u001b[0m 98.3    \u001b[0m | \u001b[0m 0.04852 \u001b[0m | \u001b[0m 0.03456 \u001b[0m | \u001b[0m 3.218   \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.5537  \u001b[0m | \u001b[0m 58.94   \u001b[0m | \u001b[0m 276.2   \u001b[0m | \u001b[0m 98.51   \u001b[0m | \u001b[0m 0.03371 \u001b[0m | \u001b[0m 0.03812 \u001b[0m | \u001b[0m 2.663   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.5459  \u001b[0m | \u001b[0m 53.48   \u001b[0m | \u001b[0m 281.2   \u001b[0m | \u001b[0m 94.81   \u001b[0m | \u001b[0m 0.000702\u001b[0m | \u001b[0m 0.04339 \u001b[0m | \u001b[0m 3.424   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.5473  \u001b[0m | \u001b[0m 53.79   \u001b[0m | \u001b[0m 272.9   \u001b[0m | \u001b[0m 97.13   \u001b[0m | \u001b[0m 0.01643 \u001b[0m | \u001b[0m 0.04312 \u001b[0m | \u001b[0m 2.298   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.5535  \u001b[0m | \u001b[0m 51.55   \u001b[0m | \u001b[0m 272.8   \u001b[0m | \u001b[0m 99.4    \u001b[0m | \u001b[0m 0.04439 \u001b[0m | \u001b[0m 0.01526 \u001b[0m | \u001b[0m 4.086   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.5442  \u001b[0m | \u001b[0m 54.38   \u001b[0m | \u001b[0m 278.6   \u001b[0m | \u001b[0m 95.97   \u001b[0m | \u001b[0m 0.03951 \u001b[0m | \u001b[0m 0.03982 \u001b[0m | \u001b[0m 4.001   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.5475  \u001b[0m | \u001b[0m 52.77   \u001b[0m | \u001b[0m 279.4   \u001b[0m | \u001b[0m 96.34   \u001b[0m | \u001b[0m 0.01725 \u001b[0m | \u001b[0m 0.00542 \u001b[0m | \u001b[0m 4.583   \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.5503  \u001b[0m | \u001b[0m 61.49   \u001b[0m | \u001b[0m 270.8   \u001b[0m | \u001b[0m 99.22   \u001b[0m | \u001b[0m 0.02679 \u001b[0m | \u001b[0m 0.02369 \u001b[0m | \u001b[0m 7.088   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.5548  \u001b[0m | \u001b[0m 53.53   \u001b[0m | \u001b[0m 274.8   \u001b[0m | \u001b[0m 98.61   \u001b[0m | \u001b[0m 0.02109 \u001b[0m | \u001b[0m 0.01255 \u001b[0m | \u001b[0m 1.714   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.5522  \u001b[0m | \u001b[0m 52.54   \u001b[0m | \u001b[0m 278.3   \u001b[0m | \u001b[0m 98.05   \u001b[0m | \u001b[0m 0.02653 \u001b[0m | \u001b[0m 0.02722 \u001b[0m | \u001b[0m 2.261   \u001b[0m |\n",
      "| \u001b[0m 30      \u001b[0m | \u001b[0m 0.5468  \u001b[0m | \u001b[0m 45.56   \u001b[0m | \u001b[0m 275.7   \u001b[0m | \u001b[0m 93.93   \u001b[0m | \u001b[0m 0.004371\u001b[0m | \u001b[0m 0.03825 \u001b[0m | \u001b[0m 6.741   \u001b[0m |\n",
      "| \u001b[0m 31      \u001b[0m | \u001b[0m 0.547   \u001b[0m | \u001b[0m 60.15   \u001b[0m | \u001b[0m 278.2   \u001b[0m | \u001b[0m 97.21   \u001b[0m | \u001b[0m 0.03667 \u001b[0m | \u001b[0m 0.01575 \u001b[0m | \u001b[0m 2.056   \u001b[0m |\n",
      "| \u001b[0m 32      \u001b[0m | \u001b[0m 0.5472  \u001b[0m | \u001b[0m 54.16   \u001b[0m | \u001b[0m 280.2   \u001b[0m | \u001b[0m 90.1    \u001b[0m | \u001b[0m 0.0373  \u001b[0m | \u001b[0m 0.006913\u001b[0m | \u001b[0m 2.72    \u001b[0m |\n",
      "| \u001b[0m 33      \u001b[0m | \u001b[0m 0.5403  \u001b[0m | \u001b[0m 52.43   \u001b[0m | \u001b[0m 277.3   \u001b[0m | \u001b[0m 98.84   \u001b[0m | \u001b[0m 0.01445 \u001b[0m | \u001b[0m 0.004712\u001b[0m | \u001b[0m 4.389   \u001b[0m |\n",
      "| \u001b[0m 34      \u001b[0m | \u001b[0m 0.5422  \u001b[0m | \u001b[0m 50.15   \u001b[0m | \u001b[0m 273.8   \u001b[0m | \u001b[0m 92.72   \u001b[0m | \u001b[0m 0.04256 \u001b[0m | \u001b[0m 0.001951\u001b[0m | \u001b[0m 5.326   \u001b[0m |\n",
      "| \u001b[0m 35      \u001b[0m | \u001b[0m 0.5464  \u001b[0m | \u001b[0m 44.84   \u001b[0m | \u001b[0m 271.7   \u001b[0m | \u001b[0m 93.67   \u001b[0m | \u001b[0m 0.0273  \u001b[0m | \u001b[0m 0.04041 \u001b[0m | \u001b[0m 5.869   \u001b[0m |\n",
      "| \u001b[0m 36      \u001b[0m | \u001b[0m 0.5484  \u001b[0m | \u001b[0m 53.68   \u001b[0m | \u001b[0m 275.2   \u001b[0m | \u001b[0m 97.86   \u001b[0m | \u001b[0m 0.03408 \u001b[0m | \u001b[0m 0.02394 \u001b[0m | \u001b[0m 1.056   \u001b[0m |\n",
      "| \u001b[0m 37      \u001b[0m | \u001b[0m 0.5509  \u001b[0m | \u001b[0m 49.3    \u001b[0m | \u001b[0m 66.91   \u001b[0m | \u001b[0m 37.44   \u001b[0m | \u001b[0m 0.04022 \u001b[0m | \u001b[0m 0.03022 \u001b[0m | \u001b[0m 2.812   \u001b[0m |\n",
      "| \u001b[0m 38      \u001b[0m | \u001b[0m 0.5476  \u001b[0m | \u001b[0m 57.02   \u001b[0m | \u001b[0m 274.2   \u001b[0m | \u001b[0m 99.21   \u001b[0m | \u001b[0m 0.01853 \u001b[0m | \u001b[0m 0.02217 \u001b[0m | \u001b[0m 3.112   \u001b[0m |\n",
      "| \u001b[0m 39      \u001b[0m | \u001b[0m 0.5473  \u001b[0m | \u001b[0m 81.29   \u001b[0m | \u001b[0m 244.3   \u001b[0m | \u001b[0m 85.73   \u001b[0m | \u001b[0m 0.02891 \u001b[0m | \u001b[0m 0.001901\u001b[0m | \u001b[0m 8.543   \u001b[0m |\n",
      "| \u001b[0m 40      \u001b[0m | \u001b[0m 0.5492  \u001b[0m | \u001b[0m 59.25   \u001b[0m | \u001b[0m 273.0   \u001b[0m | \u001b[0m 99.23   \u001b[0m | \u001b[0m 0.003597\u001b[0m | \u001b[0m 0.02753 \u001b[0m | \u001b[0m 5.55    \u001b[0m |\n",
      "| \u001b[0m 41      \u001b[0m | \u001b[0m 0.5488  \u001b[0m | \u001b[0m 55.69   \u001b[0m | \u001b[0m 276.6   \u001b[0m | \u001b[0m 97.04   \u001b[0m | \u001b[0m 0.01792 \u001b[0m | \u001b[0m 0.005025\u001b[0m | \u001b[0m 1.726   \u001b[0m |\n",
      "| \u001b[0m 42      \u001b[0m | \u001b[0m 0.5539  \u001b[0m | \u001b[0m 52.82   \u001b[0m | \u001b[0m 64.59   \u001b[0m | \u001b[0m 35.74   \u001b[0m | \u001b[0m 0.0297  \u001b[0m | \u001b[0m 0.01878 \u001b[0m | \u001b[0m 3.797   \u001b[0m |\n",
      "| \u001b[0m 43      \u001b[0m | \u001b[0m 0.5484  \u001b[0m | \u001b[0m 50.57   \u001b[0m | \u001b[0m 272.6   \u001b[0m | \u001b[0m 99.49   \u001b[0m | \u001b[0m 0.02129 \u001b[0m | \u001b[0m 0.04721 \u001b[0m | \u001b[0m 4.556   \u001b[0m |\n",
      "| \u001b[0m 44      \u001b[0m | \u001b[0m 0.5456  \u001b[0m | \u001b[0m 51.6    \u001b[0m | \u001b[0m 277.6   \u001b[0m | \u001b[0m 96.4    \u001b[0m | \u001b[0m 0.009696\u001b[0m | \u001b[0m 0.01002 \u001b[0m | \u001b[0m 3.467   \u001b[0m |\n",
      "| \u001b[0m 45      \u001b[0m | \u001b[0m 0.5451  \u001b[0m | \u001b[0m 13.26   \u001b[0m | \u001b[0m 217.7   \u001b[0m | \u001b[0m 53.67   \u001b[0m | \u001b[0m 0.04106 \u001b[0m | \u001b[0m 0.04113 \u001b[0m | \u001b[0m 7.825   \u001b[0m |\n",
      "| \u001b[0m 46      \u001b[0m | \u001b[0m 0.5455  \u001b[0m | \u001b[0m 10.96   \u001b[0m | \u001b[0m 189.7   \u001b[0m | \u001b[0m 34.26   \u001b[0m | \u001b[0m 0.0389  \u001b[0m | \u001b[0m 0.04122 \u001b[0m | \u001b[0m 6.112   \u001b[0m |\n",
      "| \u001b[0m 47      \u001b[0m | \u001b[0m 0.5463  \u001b[0m | \u001b[0m 49.59   \u001b[0m | \u001b[0m 73.04   \u001b[0m | \u001b[0m 58.14   \u001b[0m | \u001b[0m 0.04787 \u001b[0m | \u001b[0m 0.02629 \u001b[0m | \u001b[0m 5.014   \u001b[0m |\n",
      "| \u001b[0m 48      \u001b[0m | \u001b[0m 0.5429  \u001b[0m | \u001b[0m 30.19   \u001b[0m | \u001b[0m 138.6   \u001b[0m | \u001b[0m 71.49   \u001b[0m | \u001b[0m 0.02398 \u001b[0m | \u001b[0m 0.04259 \u001b[0m | \u001b[0m 7.191   \u001b[0m |\n",
      "| \u001b[0m 49      \u001b[0m | \u001b[0m 0.5491  \u001b[0m | \u001b[0m 54.45   \u001b[0m | \u001b[0m 276.8   \u001b[0m | \u001b[0m 98.3    \u001b[0m | \u001b[0m 0.002272\u001b[0m | \u001b[0m 0.0479  \u001b[0m | \u001b[0m 1.87    \u001b[0m |\n",
      "| \u001b[0m 50      \u001b[0m | \u001b[0m 0.5505  \u001b[0m | \u001b[0m 33.59   \u001b[0m | \u001b[0m 231.8   \u001b[0m | \u001b[0m 92.23   \u001b[0m | \u001b[0m 0.01569 \u001b[0m | \u001b[0m 0.03569 \u001b[0m | \u001b[0m 3.278   \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 51      \u001b[0m | \u001b[0m 0.5505  \u001b[0m | \u001b[0m 53.39   \u001b[0m | \u001b[0m 64.61   \u001b[0m | \u001b[0m 35.35   \u001b[0m | \u001b[0m 0.04419 \u001b[0m | \u001b[0m 0.01687 \u001b[0m | \u001b[0m 2.733   \u001b[0m |\n",
      "| \u001b[0m 52      \u001b[0m | \u001b[0m 0.5495  \u001b[0m | \u001b[0m 51.72   \u001b[0m | \u001b[0m 266.0   \u001b[0m | \u001b[0m 58.49   \u001b[0m | \u001b[0m 0.0294  \u001b[0m | \u001b[0m 0.03087 \u001b[0m | \u001b[0m 6.534   \u001b[0m |\n",
      "| \u001b[0m 53      \u001b[0m | \u001b[0m 0.5529  \u001b[0m | \u001b[0m 78.98   \u001b[0m | \u001b[0m 148.4   \u001b[0m | \u001b[0m 85.79   \u001b[0m | \u001b[0m 0.02705 \u001b[0m | \u001b[0m 0.03893 \u001b[0m | \u001b[0m 2.976   \u001b[0m |\n",
      "| \u001b[0m 54      \u001b[0m | \u001b[0m 0.5506  \u001b[0m | \u001b[0m 24.31   \u001b[0m | \u001b[0m 67.34   \u001b[0m | \u001b[0m 99.75   \u001b[0m | \u001b[0m 0.005698\u001b[0m | \u001b[0m 0.0489  \u001b[0m | \u001b[0m 7.804   \u001b[0m |\n",
      "| \u001b[0m 55      \u001b[0m | \u001b[0m 0.554   \u001b[0m | \u001b[0m 50.91   \u001b[0m | \u001b[0m 257.9   \u001b[0m | \u001b[0m 89.12   \u001b[0m | \u001b[0m 0.04011 \u001b[0m | \u001b[0m 0.01772 \u001b[0m | \u001b[0m 6.658   \u001b[0m |\n",
      "| \u001b[0m 56      \u001b[0m | \u001b[0m 0.5474  \u001b[0m | \u001b[0m 61.26   \u001b[0m | \u001b[0m 271.0   \u001b[0m | \u001b[0m 99.88   \u001b[0m | \u001b[0m 0.01817 \u001b[0m | \u001b[0m 0.01324 \u001b[0m | \u001b[0m 8.779   \u001b[0m |\n",
      "| \u001b[0m 57      \u001b[0m | \u001b[0m 0.5505  \u001b[0m | \u001b[0m 53.59   \u001b[0m | \u001b[0m 275.1   \u001b[0m | \u001b[0m 99.43   \u001b[0m | \u001b[0m 0.003953\u001b[0m | \u001b[0m 0.04776 \u001b[0m | \u001b[0m 3.516   \u001b[0m |\n",
      "| \u001b[0m 58      \u001b[0m | \u001b[0m 0.5427  \u001b[0m | \u001b[0m 68.44   \u001b[0m | \u001b[0m 117.2   \u001b[0m | \u001b[0m 45.7    \u001b[0m | \u001b[0m 0.02122 \u001b[0m | \u001b[0m 0.004322\u001b[0m | \u001b[0m 1.425   \u001b[0m |\n",
      "| \u001b[0m 59      \u001b[0m | \u001b[0m 0.5505  \u001b[0m | \u001b[0m 26.62   \u001b[0m | \u001b[0m 178.6   \u001b[0m | \u001b[0m 86.04   \u001b[0m | \u001b[0m 0.02705 \u001b[0m | \u001b[0m 0.02903 \u001b[0m | \u001b[0m 7.307   \u001b[0m |\n",
      "| \u001b[0m 60      \u001b[0m | \u001b[0m 0.5506  \u001b[0m | \u001b[0m 26.55   \u001b[0m | \u001b[0m 177.8   \u001b[0m | \u001b[0m 87.75   \u001b[0m | \u001b[0m 0.001239\u001b[0m | \u001b[0m 0.04525 \u001b[0m | \u001b[0m 8.275   \u001b[0m |\n",
      "| \u001b[0m 61      \u001b[0m | \u001b[0m 0.5493  \u001b[0m | \u001b[0m 56.07   \u001b[0m | \u001b[0m 281.8   \u001b[0m | \u001b[0m 92.35   \u001b[0m | \u001b[0m 0.01182 \u001b[0m | \u001b[0m 0.02597 \u001b[0m | \u001b[0m 2.154   \u001b[0m |\n",
      "| \u001b[0m 62      \u001b[0m | \u001b[0m 0.5559  \u001b[0m | \u001b[0m 37.82   \u001b[0m | \u001b[0m 187.7   \u001b[0m | \u001b[0m 83.96   \u001b[0m | \u001b[0m 0.006642\u001b[0m | \u001b[0m 0.005022\u001b[0m | \u001b[0m 3.547   \u001b[0m |\n",
      "| \u001b[0m 63      \u001b[0m | \u001b[0m 0.5485  \u001b[0m | \u001b[0m 20.13   \u001b[0m | \u001b[0m 112.9   \u001b[0m | \u001b[0m 77.09   \u001b[0m | \u001b[0m 0.02773 \u001b[0m | \u001b[0m 0.0122  \u001b[0m | \u001b[0m 3.76    \u001b[0m |\n",
      "| \u001b[0m 64      \u001b[0m | \u001b[0m 0.5552  \u001b[0m | \u001b[0m 62.77   \u001b[0m | \u001b[0m 256.6   \u001b[0m | \u001b[0m 36.02   \u001b[0m | \u001b[0m 0.01255 \u001b[0m | \u001b[0m 0.0178  \u001b[0m | \u001b[0m 6.232   \u001b[0m |\n",
      "| \u001b[0m 65      \u001b[0m | \u001b[0m 0.5506  \u001b[0m | \u001b[0m 35.01   \u001b[0m | \u001b[0m 298.7   \u001b[0m | \u001b[0m 60.32   \u001b[0m | \u001b[0m 0.000531\u001b[0m | \u001b[0m 0.000465\u001b[0m | \u001b[0m 7.003   \u001b[0m |\n",
      "| \u001b[0m 66      \u001b[0m | \u001b[0m 0.5422  \u001b[0m | \u001b[0m 71.52   \u001b[0m | \u001b[0m 273.6   \u001b[0m | \u001b[0m 72.13   \u001b[0m | \u001b[0m 0.03806 \u001b[0m | \u001b[0m 0.04362 \u001b[0m | \u001b[0m 1.041   \u001b[0m |\n",
      "| \u001b[0m 67      \u001b[0m | \u001b[0m 0.5435  \u001b[0m | \u001b[0m 47.71   \u001b[0m | \u001b[0m 66.63   \u001b[0m | \u001b[0m 37.64   \u001b[0m | \u001b[0m 0.007908\u001b[0m | \u001b[0m 0.0281  \u001b[0m | \u001b[0m 3.418   \u001b[0m |\n",
      "| \u001b[0m 68      \u001b[0m | \u001b[0m 0.5482  \u001b[0m | \u001b[0m 55.69   \u001b[0m | \u001b[0m 280.5   \u001b[0m | \u001b[0m 92.74   \u001b[0m | \u001b[0m 0.04704 \u001b[0m | \u001b[0m 0.01728 \u001b[0m | \u001b[0m 1.636   \u001b[0m |\n",
      "| \u001b[0m 69      \u001b[0m | \u001b[0m 0.5483  \u001b[0m | \u001b[0m 48.89   \u001b[0m | \u001b[0m 247.3   \u001b[0m | \u001b[0m 68.62   \u001b[0m | \u001b[0m 0.005471\u001b[0m | \u001b[0m 0.04924 \u001b[0m | \u001b[0m 7.605   \u001b[0m |\n",
      "| \u001b[0m 70      \u001b[0m | \u001b[0m 0.5524  \u001b[0m | \u001b[0m 50.43   \u001b[0m | \u001b[0m 273.0   \u001b[0m | \u001b[0m 97.39   \u001b[0m | \u001b[0m 0.04251 \u001b[0m | \u001b[0m 0.02769 \u001b[0m | \u001b[0m 3.663   \u001b[0m |\n",
      "| \u001b[0m 71      \u001b[0m | \u001b[0m 0.5459  \u001b[0m | \u001b[0m 59.37   \u001b[0m | \u001b[0m 125.9   \u001b[0m | \u001b[0m 88.82   \u001b[0m | \u001b[0m 0.04405 \u001b[0m | \u001b[0m 0.01103 \u001b[0m | \u001b[0m 7.92    \u001b[0m |\n",
      "| \u001b[0m 72      \u001b[0m | \u001b[0m 0.5403  \u001b[0m | \u001b[0m 78.1    \u001b[0m | \u001b[0m 147.6   \u001b[0m | \u001b[0m 86.28   \u001b[0m | \u001b[0m 0.000191\u001b[0m | \u001b[0m 0.00153 \u001b[0m | \u001b[0m 3.076   \u001b[0m |\n",
      "| \u001b[0m 73      \u001b[0m | \u001b[0m 0.5449  \u001b[0m | \u001b[0m 73.98   \u001b[0m | \u001b[0m 80.86   \u001b[0m | \u001b[0m 93.46   \u001b[0m | \u001b[0m 0.02302 \u001b[0m | \u001b[0m 0.01414 \u001b[0m | \u001b[0m 1.905   \u001b[0m |\n",
      "| \u001b[0m 74      \u001b[0m | \u001b[0m 0.5368  \u001b[0m | \u001b[0m 9.796   \u001b[0m | \u001b[0m 149.1   \u001b[0m | \u001b[0m 37.37   \u001b[0m | \u001b[0m 0.02793 \u001b[0m | \u001b[0m 0.01876 \u001b[0m | \u001b[0m 8.496   \u001b[0m |\n",
      "| \u001b[0m 75      \u001b[0m | \u001b[0m 0.5472  \u001b[0m | \u001b[0m 38.96   \u001b[0m | \u001b[0m 187.1   \u001b[0m | \u001b[0m 84.63   \u001b[0m | \u001b[0m 0.02402 \u001b[0m | \u001b[0m 0.04299 \u001b[0m | \u001b[0m 3.515   \u001b[0m |\n",
      "| \u001b[0m 76      \u001b[0m | \u001b[0m 0.5395  \u001b[0m | \u001b[0m 24.17   \u001b[0m | \u001b[0m 67.22   \u001b[0m | \u001b[0m 99.96   \u001b[0m | \u001b[0m 0.02882 \u001b[0m | \u001b[0m 4.259e-0\u001b[0m | \u001b[0m 8.686   \u001b[0m |\n",
      "| \u001b[0m 77      \u001b[0m | \u001b[0m 0.5444  \u001b[0m | \u001b[0m 23.29   \u001b[0m | \u001b[0m 74.46   \u001b[0m | \u001b[0m 37.83   \u001b[0m | \u001b[0m 0.0286  \u001b[0m | \u001b[0m 0.01996 \u001b[0m | \u001b[0m 7.359   \u001b[0m |\n",
      "| \u001b[0m 78      \u001b[0m | \u001b[0m 0.5462  \u001b[0m | \u001b[0m 74.87   \u001b[0m | \u001b[0m 70.31   \u001b[0m | \u001b[0m 44.48   \u001b[0m | \u001b[0m 0.02937 \u001b[0m | \u001b[0m 0.02517 \u001b[0m | \u001b[0m 8.655   \u001b[0m |\n",
      "| \u001b[0m 79      \u001b[0m | \u001b[0m 0.5483  \u001b[0m | \u001b[0m 66.53   \u001b[0m | \u001b[0m 273.5   \u001b[0m | \u001b[0m 97.57   \u001b[0m | \u001b[0m 0.02495 \u001b[0m | \u001b[0m 0.01719 \u001b[0m | \u001b[0m 7.0     \u001b[0m |\n",
      "| \u001b[0m 80      \u001b[0m | \u001b[0m 0.5464  \u001b[0m | \u001b[0m 36.9    \u001b[0m | \u001b[0m 226.5   \u001b[0m | \u001b[0m 44.01   \u001b[0m | \u001b[0m 0.008852\u001b[0m | \u001b[0m 0.03104 \u001b[0m | \u001b[0m 4.635   \u001b[0m |\n",
      "| \u001b[0m 81      \u001b[0m | \u001b[0m 0.5486  \u001b[0m | \u001b[0m 53.31   \u001b[0m | \u001b[0m 65.58   \u001b[0m | \u001b[0m 36.58   \u001b[0m | \u001b[0m 0.001087\u001b[0m | \u001b[0m 0.005648\u001b[0m | \u001b[0m 4.079   \u001b[0m |\n",
      "| \u001b[0m 82      \u001b[0m | \u001b[0m 0.5461  \u001b[0m | \u001b[0m 48.01   \u001b[0m | \u001b[0m 215.4   \u001b[0m | \u001b[0m 69.77   \u001b[0m | \u001b[0m 0.03969 \u001b[0m | \u001b[0m 0.03494 \u001b[0m | \u001b[0m 6.306   \u001b[0m |\n",
      "| \u001b[0m 83      \u001b[0m | \u001b[0m 0.5536  \u001b[0m | \u001b[0m 32.28   \u001b[0m | \u001b[0m 232.1   \u001b[0m | \u001b[0m 92.39   \u001b[0m | \u001b[0m 0.002482\u001b[0m | \u001b[0m 0.037   \u001b[0m | \u001b[0m 2.945   \u001b[0m |\n",
      "| \u001b[0m 84      \u001b[0m | \u001b[0m 0.5492  \u001b[0m | \u001b[0m 55.02   \u001b[0m | \u001b[0m 276.2   \u001b[0m | \u001b[0m 98.33   \u001b[0m | \u001b[0m 0.04631 \u001b[0m | \u001b[0m 0.02975 \u001b[0m | \u001b[0m 2.979   \u001b[0m |\n",
      "| \u001b[0m 85      \u001b[0m | \u001b[0m 0.5445  \u001b[0m | \u001b[0m 49.06   \u001b[0m | \u001b[0m 67.95   \u001b[0m | \u001b[0m 39.54   \u001b[0m | \u001b[0m 0.00122 \u001b[0m | \u001b[0m 0.007252\u001b[0m | \u001b[0m 3.168   \u001b[0m |\n",
      "| \u001b[0m 86      \u001b[0m | \u001b[0m 0.5417  \u001b[0m | \u001b[0m 46.06   \u001b[0m | \u001b[0m 74.4    \u001b[0m | \u001b[0m 37.34   \u001b[0m | \u001b[0m 0.04316 \u001b[0m | \u001b[0m 0.02223 \u001b[0m | \u001b[0m 2.774   \u001b[0m |\n",
      "| \u001b[0m 87      \u001b[0m | \u001b[0m 0.5451  \u001b[0m | \u001b[0m 92.94   \u001b[0m | \u001b[0m 81.26   \u001b[0m | \u001b[0m 83.53   \u001b[0m | \u001b[0m 0.000946\u001b[0m | \u001b[0m 0.04606 \u001b[0m | \u001b[0m 6.134   \u001b[0m |\n",
      "| \u001b[0m 88      \u001b[0m | \u001b[0m 0.5522  \u001b[0m | \u001b[0m 52.69   \u001b[0m | \u001b[0m 278.7   \u001b[0m | \u001b[0m 98.69   \u001b[0m | \u001b[0m 0.01871 \u001b[0m | \u001b[0m 0.04608 \u001b[0m | \u001b[0m 2.48    \u001b[0m |\n",
      "| \u001b[0m 89      \u001b[0m | \u001b[0m 0.5458  \u001b[0m | \u001b[0m 50.36   \u001b[0m | \u001b[0m 276.0   \u001b[0m | \u001b[0m 95.48   \u001b[0m | \u001b[0m 0.01342 \u001b[0m | \u001b[0m 0.0361  \u001b[0m | \u001b[0m 4.875   \u001b[0m |\n",
      "| \u001b[0m 90      \u001b[0m | \u001b[0m 0.5444  \u001b[0m | \u001b[0m 73.76   \u001b[0m | \u001b[0m 144.9   \u001b[0m | \u001b[0m 51.02   \u001b[0m | \u001b[0m 0.008828\u001b[0m | \u001b[0m 0.04977 \u001b[0m | \u001b[0m 7.05    \u001b[0m |\n",
      "| \u001b[0m 91      \u001b[0m | \u001b[0m 0.5413  \u001b[0m | \u001b[0m 58.16   \u001b[0m | \u001b[0m 276.4   \u001b[0m | \u001b[0m 98.64   \u001b[0m | \u001b[0m 0.01059 \u001b[0m | \u001b[0m 0.03479 \u001b[0m | \u001b[0m 2.155   \u001b[0m |\n",
      "| \u001b[0m 92      \u001b[0m | \u001b[0m 0.5516  \u001b[0m | \u001b[0m 89.68   \u001b[0m | \u001b[0m 118.9   \u001b[0m | \u001b[0m 37.58   \u001b[0m | \u001b[0m 0.007873\u001b[0m | \u001b[0m 0.0417  \u001b[0m | \u001b[0m 6.418   \u001b[0m |\n",
      "| \u001b[0m 93      \u001b[0m | \u001b[0m 0.5453  \u001b[0m | \u001b[0m 79.78   \u001b[0m | \u001b[0m 194.4   \u001b[0m | \u001b[0m 42.35   \u001b[0m | \u001b[0m 0.004264\u001b[0m | \u001b[0m 0.03805 \u001b[0m | \u001b[0m 2.135   \u001b[0m |\n",
      "| \u001b[0m 94      \u001b[0m | \u001b[0m 0.5519  \u001b[0m | \u001b[0m 92.91   \u001b[0m | \u001b[0m 249.7   \u001b[0m | \u001b[0m 37.4    \u001b[0m | \u001b[0m 0.02524 \u001b[0m | \u001b[0m 0.01999 \u001b[0m | \u001b[0m 7.23    \u001b[0m |\n",
      "| \u001b[0m 95      \u001b[0m | \u001b[0m 0.5535  \u001b[0m | \u001b[0m 65.67   \u001b[0m | \u001b[0m 272.2   \u001b[0m | \u001b[0m 96.58   \u001b[0m | \u001b[0m 5.988e-0\u001b[0m | \u001b[0m 0.03294 \u001b[0m | \u001b[0m 7.962   \u001b[0m |\n",
      "| \u001b[0m 96      \u001b[0m | \u001b[0m 0.54    \u001b[0m | \u001b[0m 48.06   \u001b[0m | \u001b[0m 115.5   \u001b[0m | \u001b[0m 32.69   \u001b[0m | \u001b[0m 0.02165 \u001b[0m | \u001b[0m 0.04478 \u001b[0m | \u001b[0m 4.535   \u001b[0m |\n",
      "| \u001b[0m 97      \u001b[0m | \u001b[0m 0.5507  \u001b[0m | \u001b[0m 52.32   \u001b[0m | \u001b[0m 260.3   \u001b[0m | \u001b[0m 67.12   \u001b[0m | \u001b[0m 0.01722 \u001b[0m | \u001b[0m 0.02056 \u001b[0m | \u001b[0m 2.267   \u001b[0m |\n",
      "| \u001b[0m 98      \u001b[0m | \u001b[0m 0.5497  \u001b[0m | \u001b[0m 28.58   \u001b[0m | \u001b[0m 273.1   \u001b[0m | \u001b[0m 96.25   \u001b[0m | \u001b[0m 0.03526 \u001b[0m | \u001b[0m 0.006993\u001b[0m | \u001b[0m 3.923   \u001b[0m |\n",
      "| \u001b[0m 99      \u001b[0m | \u001b[0m 0.5432  \u001b[0m | \u001b[0m 25.4    \u001b[0m | \u001b[0m 58.61   \u001b[0m | \u001b[0m 36.73   \u001b[0m | \u001b[0m 0.04314 \u001b[0m | \u001b[0m 0.04588 \u001b[0m | \u001b[0m 1.871   \u001b[0m |\n",
      "| \u001b[0m 100     \u001b[0m | \u001b[0m 0.5469  \u001b[0m | \u001b[0m 26.76   \u001b[0m | \u001b[0m 178.1   \u001b[0m | \u001b[0m 88.14   \u001b[0m | \u001b[0m 0.02511 \u001b[0m | \u001b[0m 0.02768 \u001b[0m | \u001b[0m 8.182   \u001b[0m |\n",
      "| \u001b[0m 101     \u001b[0m | \u001b[0m 0.5429  \u001b[0m | \u001b[0m 9.181   \u001b[0m | \u001b[0m 224.6   \u001b[0m | \u001b[0m 95.56   \u001b[0m | \u001b[0m 0.0075  \u001b[0m | \u001b[0m 0.007088\u001b[0m | \u001b[0m 5.481   \u001b[0m |\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| \u001b[0m 102     \u001b[0m | \u001b[0m 0.5475  \u001b[0m | \u001b[0m 71.68   \u001b[0m | \u001b[0m 94.3    \u001b[0m | \u001b[0m 80.74   \u001b[0m | \u001b[0m 0.02608 \u001b[0m | \u001b[0m 0.01414 \u001b[0m | \u001b[0m 3.614   \u001b[0m |\n",
      "=================================================================================================\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization\n",
    "\n",
    "num_transformer = Pipeline(steps=[\n",
    "                                ('imputer', SimpleImputer(strategy = 'median')),\n",
    "                                ('scaler', RobustScaler())\n",
    "                                ])\n",
    "\n",
    "cat_transformer = Pipeline(steps=[\n",
    "                                ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "                                ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "                                ])\n",
    "    \n",
    "def search_model_xgb(num_leaves,max_depth,n_estimators,reg_alpha,reg_lambda,scale_pos_weight):\n",
    "    params = {\n",
    "         'max_depth':int(max_depth),\n",
    "         'n_estimators':int(n_estimators),\n",
    "         'reg_alpha':reg_alpha,\n",
    "         'reg_lambda':reg_lambda,\n",
    "        'scale_pos_weight':scale_pos_weight,\n",
    "         'random_state':0,\n",
    "        'use_label_encoder':False,\n",
    "        'verbosity':0\n",
    "    }\n",
    "    xgb = XGBClassifier(**params)\n",
    "    transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', num_transformer, num_cols_fe),\n",
    "            ('cat', cat_transformer, cat_cols_fe)\n",
    "        ])\n",
    "    main_pipeline = Pipeline(steps=[('transformer', transformer),\n",
    "                      ('classifier', xgb)])\n",
    "    \n",
    "    aucs = []\n",
    "    for i in range(len(data_skf['train'])):\n",
    "        model = clone(main_pipeline)\n",
    "        model.fit(data_skf['train'][i][0],data_skf['train'][i][1].values)\n",
    "        pred_proba = model.predict_proba(data_skf['val'][i][0])[:,1]\n",
    "        \n",
    "        aucs.append(roc_auc_score(data_skf['val'][i][1].values, pred_proba,average='weighted'))\n",
    "    return np.mean(aucs)\n",
    "\n",
    "xgbBO = BayesianOptimization(search_model_xgb, {'num_leaves': (25, 100),\n",
    "                                        'max_depth': (5, 100),\n",
    "                                        'n_estimators':(50,300),\n",
    "                                        'reg_alpha': (0.0, 0.05),\n",
    "                                        'reg_lambda': (0.0, 0.05),\n",
    "                                         'scale_pos_weight':(1,9)\n",
    "                                        },random_state=0)\n",
    "xgbBO.maximize(n_iter=100, init_points=2)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T15:49:57.927756Z",
     "start_time": "2021-02-15T15:49:57.919759Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': 0.5559541550075702,\n",
       " 'params': {'max_depth': 52.3160713621035,\n",
       "  'n_estimators': 278.30132378597926,\n",
       "  'num_leaves': 96.9938470728089,\n",
       "  'reg_alpha': 0.04800396971863674,\n",
       "  'reg_lambda': 0.047213768862403666,\n",
       "  'scale_pos_weight': 3.8022455273872264}}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgbBO.max\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T15:52:49.745838Z",
     "start_time": "2021-02-15T15:52:49.740811Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'boosting_type':'gbdt',\n",
    "     'max_depth': int(52.3160713621035),\n",
    "      'n_estimators': int(278.30132378597926),\n",
    "      'num_leaves': int(96.9938470728089),\n",
    "      'reg_alpha': 0.04800396971863674,\n",
    "      'reg_lambda': 0.047213768862403666,\n",
    "      'scale_pos_weight': 3.8022455273872264,\n",
    "     'random_state':0,\n",
    "        'use_label_encoder':False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-02-15T15:52:57.518245Z",
     "start_time": "2021-02-15T15:52:50.276248Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5559541550075702"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "xgb = XGBClassifier(**params)\n",
    "transformer = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_cols_fe),\n",
    "        ('cat', cat_transformer, cat_cols_fe)\n",
    "    ])\n",
    "main_pipeline = Pipeline(steps=[('transformer', transformer),\n",
    "                  ('classifier', xgb)])\n",
    "\n",
    "aucs = []\n",
    "for i in range(len(data_skf['train'])):\n",
    "    model = clone(main_pipeline)\n",
    "    model.fit(data_skf['train'][i][0],data_skf['train'][i][1].values)\n",
    "    pred_proba = model.predict_proba(data_skf['val'][i][0])[:,1]\n",
    "\n",
    "    aucs.append(roc_auc_score(data_skf['val'][i][1].values, pred_proba,average='weighted'))\n",
    "\n",
    "np.mean(aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
